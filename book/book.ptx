<?xml version="1.0" encoding="UTF-8"?>

<!--
% LaTeX source for ``Think OS: A Brief Introduction to Operating Systems''
% Copyright 2015  Allen B. Downey.

% License: Creative Commons 
% Attribution-NonCommercial-ShareAlike 4.0 International
% http://creativecommons.org/licenses/by-nc-sa/4.0/
%

-->

<!-- pretext todo:
     - \it \bf
     - convert images to svg
     - programs should be complete (includes, main, etc)
     - state diagram for process states
     - format C code consistently
     - exit(-1) is not ok
-->
<pretext xmlns:xi="http://wwww.w3.org/2001/XInclude" xml:lang="en-US">

    <docinfo>
        <document-id edition="ptx">thinkos</document-id>
        <initialism>TOS</initialism>
        <blurb shelf="Operating Systems">
            Operating systems is usually taught as advanced topic with the assumption that
            some of the students will go on to do research in this area or write part of
            an OS. This book is focuses on the low level application and embedded programmer
            with a focus on what programmers need to understand about the operating systems
            on which their programs run.
        </blurb>
        <numbering>
            <figures level="2"/>
            <exercises level="2"/>
        </numbering>
    </docinfo>

    <book xml:id="book">
        <title>Think OS</title>
        <subtitle>A Brief Introduction to Operating Systems</subtitle>

        <frontmatter xml:id="frontmatter">
            <bibinfo>
                <author>
                    <personname>Allen B. Downey</personname>
                </author>
                <copyright>
                    <year>2015</year>
                    <holder>Allen B. Downey</holder>
                    <minilicense>Createive Commons BY-NC-SA 4.0 License</minilicense>
                    <shortlicense>
                        This ebook is licensed under
                        <url href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0
                        <icon name="cc"/> <icon name="cc-by"/> <icon name="cc-nc"/> <icon name="cc-sa"/>
                        </url>
                    </shortlicense>
                </copyright>
            </bibinfo>

            <titlepage>
                <titlepage-items/>
            </titlepage>

            <colophon xml:id="front-colophon">
                <colophon-items/>
            </colophon>

            <preface>
                <title>Preface</title>
                
                <p>In many computer science programs, Operating Systems is an advanced
                topic.  By the time students take it, they know how to program
                in C, and they have probably taken a class in Computer Architecture.
                Usually the goal of the class is to expose students to the design
                and implementation of operating systems, with the implied assumption
                that some of them will do research in this area, or write part of
                an OS.</p>

                <p>This book is intended for a different audience, and it has different
                goals.  I developed it for a class at Olin College called Software
                Systems.</p>

                <p>Most students taking this class learned to program in Python,
                so one of the goals is to help them learn C.
                For that part of the class, I use Griffiths and Griffiths,
                <pubtitle>Head First C</pubtitle>, from O'Reilly Media.  This book is
                meant to complement that one.</p>

                <p>Few of my students will ever write an operating system, but many of
                them will write low-level applications in C or work on embedded
                systems.  My class includes material from operating systems, networks,
                databases, and embedded systems, but it emphasizes the topics
                programmers need to know.</p>

                <p>This book does not assume that you have studied Computer Architecture.
                As we go along, I will explain what we need.</p>

                <p>If this book is successful, it should give you a better understanding
                of what is happening when programs run, and what you can do to make
                them run better and faster.</p>

                <p><xref ref="compilation"/> explains some of the differences between compiled and
                interpreted languages, with some insight into how compilers work.
                Recommended reading: <pubtitle>Head First C</pubtitle> Chapter<nbsp/>1.</p>

                <p><xref ref="processes"/> explains how the operating system uses processes to
                protect running programs from interfering with each other.</p>

                <p><xref ref="virtualmemory"/> explains virtual memory and address translation.
                Recommended reading: <pubtitle>Head First C</pubtitle> Chapter<nbsp/>2.</p>

                <p><xref ref="filesystems"/> is about file systems and data streams.
                Recommended reading: <pubtitle>Head First C Chapter 3</pubtitle>.</p>

                <p><xref ref="bitsandbytes"/> describes how numbers, letters, and other values are
                encoded, and presents the bitwise operators.</p>

                <p><xref ref="memorymanagement"/> explains how to use dynamic memory management, and how
                it works.
                Recommended reading: <pubtitle>Head First C</pubtitle> Chapter 6.</p>

                <p><xref ref="caching"/> is about caching and the memory hierarchy.</p>

                <p><xref ref="multitasking"/> is about multitasking and scheduling.</p>

                <p><xref ref="threads"/> is about POSIX threads and mutexes.
                Recommended reading: <pubtitle>Head First C</pubtitle> Chapter<nbsp/>12 and
                <pubtitle>Little Book of Semaphores</pubtitle> Chapters 1 and 2.</p>

                <p><xref ref="conditionvariables"/> is about POSIX condition variables
                and the producer/consumer problem.
                Recommended reading: <pubtitle>Little Book of Semaphores</pubtitle>
                Chapters<nbsp/>3 and 4.</p>

                <p><xref ref="semaphores-in-c"/> is about using POSIX semaphores and implementing
                semaphores in C.</p>
                
                <paragraphs>
                    <title>A note on this draft</title>
                    <p>
                        The current version of this book is an early draft.  While I am
                        working on the text, I have not yet included the figures.  So
                        there are a few places where, I'm sure, the explanation will be
                        greatly improved when the figures are ready.
                    </p>
                </paragraphs>

                <paragraphs xml:id="code">
                    <title>Using the code</title>

                    <p>
                        Example code for this book is available from
                        <url href="https://github.com/AllenDowney/ThinkOS">GitHub</url>.
                        Git is a version
                        control system that allows you to keep track of the files that
                        make up a project.  A collection of files under Git's control is
                        called a <term>repository</term>.  GitHub is a hosting service that provides
                        storage for Git repositories and a convenient web interface.
                        <idx>repository</idx>
                        <idx>Git</idx>
                        <idx>GitHub</idx>
                    </p>

                    <p>
                        The GitHub homepage for my repository provides several ways to
                        work with the code:

                        <ul>

                            <li>You can create a copy of my repository
                            on GitHub by pressing the <c>Fork</c> button.  If you don't already
                            have a GitHub account, you'll need to create one.  After forking, you'll
                            have your own repository on GitHub that you can use to keep track
                            of code you write while working on this book.  Then you can
                            clone the repo, which means that you copy the files
                            to your computer.<idx>fork</idx> </li>

                            <li>Or you could clone
                            my repository.  You don't need a GitHub account to do this, but you
                            won't be able to write your changes back to GitHub.
                            <idx>clone</idx></li>

                            <li>If you don't want to use Git at all, you can download the files
                            in a Zip file using the button in the lower-right corner of the
                            GitHub page.</li>

                        </ul>
                    </p>
                </paragraphs>

                <paragraphs>
                    <title>Contributor List</title>

                    <p>
                        If you have a suggestion or correction, please send email to 
                        <email>downey@allendowney.com</email>.  If I make a change based on your
                        feedback, I will add you to the contributor list
                        (unless you ask to be omitted).
                        <idx>contributors</idx>
                    </p>

                    <p>
                        If you include at least part of the sentence the
                        error appears in, that makes it easy for me to search.  Page and
                        section numbers are fine, too, but not quite as easy to work with.
                        Thanks!
                    </p>


                    <p><ul>
                        <li>I am grateful to the students in Software Systems at Olin
                        College, who tested an early draft of this book in Spring 2014.
                        They corrected many errors and made many helpful suggestions.
                        I appreciate their pioneering spirit!</li>

                        <li>James P Giannoules spotted a copy-and-paste error.</li>

                        <li>Andy Engle knows the difference between GB and GiB.</li>

                        <li>Aashish Karki noted some broken syntax.</li>
                    </ul></p>

                    <p>
                        Other people who found typos and errors include
                        Jim Tyson, Donald Robertson, Jeremy Vermast, Yuzhong Huang, Ian Hill.
                    </p>
                </paragraphs>
            </preface>
        </frontmatter>
        
        <chapter xml:id="compilation">
            <title>Compilation</title>

            <section>
                <title>Compiled and interpreted languages</title>

                <p>
                    People often describe programming languages as either compiled or interpreted.
                    <q>Compiled</q> means that programs are translated into machine language and
                    then executed by hardware; <q>interpreted</q> means that programs
                    are read and executed by a software interpreter.
                    Usually C is considered a compiled language and Python is
                    considered an interpreted language.  But the distinction is not always
                    clear-cut.
                </p>
                <p>
                    First, many languages can be either compiled or interpreted.  For
                    example, there are C interpreters and Python compilers.
                    Second, there are languages like Java that use a hybrid
                    approach, compiling programs into an intermediate language and then
                    running the translated program in an interpreter.  Java uses an
                    intermediate language called Java bytecode, which is similar to
                    machine language, but it is executed by a software interpreter, the
                    Java virtual machine (JVM).
                </p>
                <p>
                    So being compiled or interpreted is not an intrinsic
                    characteristic of a language; nevertheless, there are some general
                    differences between compiled and interpreted languages.
                </p>
            </section>

            <section>
                <title>Static types</title>

                <p>
                    Many interpreted languages support dynamic types, but compiled
                    languages are usually limited to static types.  In a statically-typed
                    language, you can tell by looking at the program what type each
                    variable refers to.  In a dynamically-typed language,
                    you don't always know the type of a variable until the
                    program is running.  In general, <term>static</term> refers to things that
                    happen at compile time (while a program is being compiled), and
                    <term>dynamic</term> refers to things that happen
                    at run time (while a program is running).
                </p>
                
                <p>
                    For example, in Python you can write a function like this:
                </p>
                <program language="python">
                    def add(x, y):
                        return x + y
                </program>

                <p>
                    Looking at this code, you can't tell what type <c>x</c> and <c>y</c>
                    will refer to at run time.  This function might be called several
                    times, each time with values with different types.  Any values that
                    support the addition operator will work; any other types will cause an
                    exception or <term>runtime error</term>.
                </p>

                <p>
                    In C you would write the same function like this:
                </p>
                <program language="cpp">
                    int add(int x, int y) {
                        return x + y;
                    }
                </program>

                <p>
                    The first line of the function includes <term>type declarations</term> for the
                    parameters and the return value: <c>x</c> and <c>y</c> are declared to
                    be integers, which means that we can check at compile time
                    whether the addition operator is legal for this type (it is).  The
                    return value is also declared to be an integer.
                </p>

                <p>
                    Because of these declarations, when this function is called elsewhere
                    in the program, the compiler can check whether the arguments provided
                    have the right type, and whether the return value is used correctly.
                </p>
                
                <p>
                    These checks happen before the program starts executing, so errors can
                    be found earlier.  More importantly, errors can be found in parts
                    of the program that have never run.  Furthermore, these checks don't
                    have to happen at run time, which is one of the reasons compiled
                    languages generally run faster than interpreted languages.
                </p>
                
                <p>
                    Declaring types at compile time also saves space.  In dynamic
                    languages, variable names are stored in memory while the program runs,
                    and they are often accessible by the program.  For example, in Python
                    the built-in function <c>locals</c> returns a dictionary that contains
                    variable names and their values.  Here's an example in a Python
                    interpreter:
                </p>
                <console><output><![CDATA[
                        >>> x = 5
                        >>> print locals()
                        {'x': 5, '__builtins__': <module '__builtin__' (built-in)>,
                        '__name__': '__main__', '__doc__': None, '__package__': None}
                ]]></output></console>

                <p>
                    This shows that the name of the variable is stored in memory while
                    the program is running (along with some other values that are part
                    of the default runtime environment).
                </p>

                <p>
                    In compiled languages, variable names exist at compile-time but not at
                    run time.  The compiler chooses a location for each variable and
                    records these locations as part of the compiled program.
                    <fn>This is a simplification; we will go into more detail later.</fn>
                    The location of a variable is called its <term>address</term>.  At run time, the
                    value of each variable is stored at its address, but the names of the
                    variables are not stored at all (unless they are added by the compiler
                    for purposes of debugging).
                </p>

<!--
%This difference is reflected in the way people draw diagrams for
%different languages.  In Python, every variable is a reference to
%a value, so the usual diagram shows a name with an arrow pointing
%to its value.  In C, a variable is the name of a location in memory
%that stores a value, so the usual diagram is a name next to a
%box that contains a value.

%\begin{figure}
% descriptive.py
%\centerline{\includegraphics[width=2.5in]{figs/variables.pdf}}
%\caption{Diagrams that represent variables in Python (left) and
%C (right).}
%\label{variables}
%\end{figure}
-->

                </section>
                <section>
                    <title>The compilation process</title>

                    <p>
                        As a programmer, you should have a mental model of what happens
                        during compilation.  If you understand the process, it will help
                        you interpret error messages, debug your code, and avoid
                        common pitfalls.
                    </p>

                    <p>
                        The steps of compilation are:
                        <ol>
                            <li><term>Preprocessing</term>: C is one of several languages that include
                            <term>preprocessing directives</term> that take effect before the program is
                            compiled.  For example, the <c>#include</c> directive causes the
                            source code from another file to be inserted at the location of the
                            directive.</li>

                            <li><term>Parsing</term>: During parsing, the compiler reads the source code and
                            builds an internal representation of the program, called an
                            <term>abstract syntax tree</term>.
                            Errors detected during this step are generally syntax errors.</li>

                            <li><term>Static checking</term>: The compiler checks whether variables and
                            values have the right type, whether functions are called with the
                            right number and type of arguments, etc.  Errors detected during
                            this step are sometimes called <term>static semantic</term> errors.</li>

                            <li><term>Code generation</term>: The compiler reads the internal representation
                            of the program and generates machine code or byte code.</li>

                            <li><term>Linking</term>: If the program uses values and functions defined in a
                            library, the compiler has to find the appropriate library and
                            include the required code.</li>

                            <li><term>Optimization</term>: At several points in the process, the compiler
                            can transform the program to generate code that runs faster or
                            uses less space.  Most optimizations are simple changes that eliminate
                            obvious waste, but some compilers perform sophisticated analyses and
                            transformations.</li>
                        </ol>
                    </p>
                    <p>
                        Normally when you run <c>gcc</c>, it runs all of these steps and
                        generates an executable file.  For example, here is a minimal C
                        program:
                    </p>
                    <program language="cpp"><![CDATA[
                            #include <stdio.h>
                            int main() {
                                printf("Hello World\n");
                            }
                    ]]></program>
                    <p>
                        If you save this code in a file called <c>hello.c</c>, you can compile
                        and run it like this:
                    </p>
                    <console>
                        <input>gcc hello.c</input><input>./a.out</input>
                        <output>Hello World</output>
                    </console>
                    <p>
                        By default, <c>gcc</c> stores the executable code in a file
                        called <c>a.out</c> (which originally stood for <q>assembler output</q>).
                        The second line runs the executable.  The prefix <c>./</c> tells
                        the shell to look for it in the current directory.
                    </p>

                    <p>
                        It is usually a good idea to use the <c>-o</c> flag to provide a
                        better name for the executable:
                    </p>
                    <console>
                        <input>gcc hello.c -o hello</input>
                        <input>./hello</input>
                        <output>Hello World</output>
                    </console>
                </section>

                <section>
                    <title>Object code</title>

                    <p>
                        The <c>-c</c> flag tells <c>gcc</c> to compile the program and
                        generate machine code, but not to link it or generate an executable:
                    </p>
                    <console>
                        <input>gcc hello.c -c</input>
                    </console>
                    <p>
                        The result is a file named <c>hello.o</c>, where the <c>o</c> stands for
                        <term>object code</term>, which is the compiled program.  Object code is not
                        executable, but it can be linked into an executable.
                    </p>

                    <p>
                        The UNIX command <c>nm</c> reads an object file and generates
                        information about the names it defines and uses.  For example:
                    </p>
                    <console>
                        <input>nm hello.o</input>
                        <output>
                                0000000000000000 T main
                                                 U puts
                        </output>
                    </console>
                    <p>
                        This output indicates that <c>hello.o</c> defines the name <c>main</c>
                        and uses a function named <c>puts</c>, which stands for <q>put string</q>.
                        In this example, <c>gcc</c> performs an optimization by replacing
                        <c>printf</c>, which is a large and complicated function, with
                        <c>puts</c>, which is relatively simple.
                    </p>

                    <p>
                        You can control how much optimization <c>gcc</c> does with
                        the <c>-O</c> flag.  By default, it does very little optimization, which
                        can make debugging easier.  The option <c>-O1</c> turns on the most
                        common and safe optimizations.  Higher numbers turn on additional
                        optimizations that require longer compilation time.
                    </p>

                    <p>
                        In theory, optimization should not change the behavior of the program,
                        other than to speed it up.  But if your program has a subtle bug,
                        you might find that optimization makes the bug appear or disappear.
                        It is usually a good idea to turn off optimization while you are developing
                        new code.  Once the program is working and passing appropriate tests,
                        you can turn on optimization and confirm that the tests still pass.
                    </p>
                </section>

                <section>
                    <title>Assembly code</title>

                    <p>
                        Similar to the <c>-c</c> flag, the <c>-S</c> flag tells <c>gcc</c>
                        to compile the program and generate assembly code, which is basically
                        a human-readable form of machine code.
                    </p>
                    <console>
                        <input>gcc hello.c -S</input>
                    </console>
                    <p>
                        The result is a file named <c>hello.s</c>, which might look something like this:
                    </p>
                    <console><output>
        .file        "hello.c"
        .section     .rodata
.LC0:
        .string      "Hello World"
        .text
        .globl       main
        .type        main, @function
main:
.LFB0:
        .cfi_startproc
        pushq %rbp
        .cfi_def_cfa_offset 16
        .cfi_offset 6, -16
        movq %rsp, %rbp
        .cfi_def_cfa_register 6
        movl $.LC0, %edi
        call puts
        movl $0, %eax
        popq %rbp
        .cfi_def_cfa 7, 8
        ret
        .cfi_endproc
.LFE0:
        .size        main, .-main
        .ident       "GCC: (Ubuntu/Linaro 4.7.3-1ubuntu1) 4.7.3"
        .section     .note.GNU-stack,"",@progbits
                    </output></console>
                    
                    <p>
                        <c>gcc</c> is usually configured to generate code for the machine you
                        are running on, so for me it generates x86 assembly language,
                        which runs on a wide variety of processors from Intel, AMD, and
                        others.  If you are running on a different architecture, you might
                        see different code.
                    </p>
                </section>

                <section>
                    <title>Preprocessing</title>

                    <p>
                        Taking another step backward through the compilation process, you
                        can use the <c>-E</c> flag to run the preprocessor only:
                    </p>
                    <console>
                        <input>gcc hello.c -E</input>
                    </console>
                    <p>
                        The result is the output from the preprocessor.  In this example,
                        it contains the included code from <c>stdio.h</c>, and all the files
                        included from <c>stdio.h</c>, and all the files included from those
                        files, and so on.  On my machine, the total is more than 800 lines
                        of code.  Since almost every C program includes <c>stdio.h</c>, those
                        800 lines of code get compiled a lot.  If, like many C programs,
                        you also include <c>stdlib.h</c>, the result is more than 1800 lines
                        of code.
                    </p>
                </section>

                <section>
                    <title>Understanding errors</title>

                    <p>
                        Now that we know the steps in the compilation process, it is easier
                        to understand error messages.  For example, if there is an error
                        in a <c>#include</c> directive, you'll get a message from the
                        preprocessor:
                    </p>
                    <console><output>
hello.c:1:20: fatal error: stdioo.h: No such file or directory
compilation terminated.
                    </output></console>

                    <p>
                        If there's a syntax error, you get a message from the compiler:
                    </p>
                    <console><output>
hello.c: In function 'main':
hello.c:6:1: error: expected ';' before '}' token
                    </output></console>

                    <p>
                        If you use a function that's not defined in any of the standard
                        libraries, you get a message from the linker:
                    </p>
                    <console><output>
/tmp/cc7iAUbN.o: In function `main':
hello.c:(.text+0xf): undefined reference to `printff'
collect2: error: ld returned 1 exit status
                    </output></console>
                    
                    <p>
                        <c>ld</c> is the name of the UNIX linker, so named because <q>loading</q>
                        is another step in the compilation process that is closely related
                        to linking.
                    </p>

                    <p>
                        Once the program starts, C does very little runtime checking,
                        so there are only a few runtime errors you are likely to see.  If you
                        divide by zero, or perform another illegal floating-point operation, you
                        will get a <q>Floating point exception.</q>  And if you try to read or
                        write an incorrect location in memory, you will get a <q>Segmentation
                        fault.</q>
                    </p>

                </section>
                <!-- % TODO: -Wall and lint -->
        </chapter>

        <chapter xml:id="processes">
            <title>Processes</title>

            <section>
                <title>Abstraction and virtualization</title>

                <p>
                    Before we talk about processes, I want to define a few words:

                    <dl width="narrow">
                        <li>
                            <title>Abstraction</title>
                            <p>
                                An abstraction is a simplified representation
                                of something complicated.  For example, if you drive a car, you
                                understand that when you turn the wheel left, the car goes left,
                                and vice versa.  Of course, the steering wheel is connected to
                                a sequence of mechanical and (often) hydraulic systems that turn
                                the wheels, and the wheels interact with the road in ways that
                                can be complex, but as a driver, you normally don't have to think
                                about any of those details.  You can get along very well with
                                a simple mental model of steering.  Your mental model is an
                                abstraction.
                            </p>
                            <p>
                                Similarly, when you use a web browser, you understand that when
                                you click on a link, the browser displays the page the link refers
                                to.  The software and network communication that make that possible
                                are complex, but as a user, you don't have to know the
                                details.
                            </p>
                            <p>
                                A large part of software engineering is designing abstractions like
                                these that allow users and other programmers to use powerful
                                and complicated systems without having to know about the details
                                of their implementation.
                            </p>
                        </li>
                        <li>
                            <title>Virtualization</title>
                            <p>
                                An important kind of abstraction is
                                virtualization, which is the process of creating a desirable
                                illusion.
                            </p>
                            <p>
                                For example, many public libraries participate in inter-library
                                collaborations that allow them to borrow books from each other.
                                When I request a book, sometimes the book is on the shelf at my
                                local library, but other times it has to be transferred from another
                                collection.  Either way, I get a notification when it is available
                                for pickup.  I don't need to know where it came from, and I don't
                                need to know which books my library has.  As a whole, the system
                                creates the illusion that my library has every book in the world.
                            </p>
                            <p>
                                The collection physically located at my local library might be small,
                                but the collection available to me virtually includes every book
                                in the inter-library collaboration.
                            </p>
                            <p>
                                As another example, most computers are only connected to one
                                network, but that network is connected to others, and so on.  What
                                we call the Internet is a collection of networks and a set of
                                protocols that forward packets from one network to the next.
                                From the point of view of a user or programmer, the system behaves
                                as if every computer on the Internet is connected to every other
                                computer.  The number of physical connections is small, but the
                                number of virtual connections is very large.
                            </p>
                        </li>
                    </dl>
                </p>

                <p>
                    The word <q>virtual</q> is often used in the context of a virtual
                    machine, which is software that creates the illusion of a dedicated
                    computer running a particular operating system, when in reality
                    the virtual machine might be running, along with many other virtual
                    machines, on a computer running a different operating system.
                </p>
                <p>
                    In the context of virtualization, we sometimes call what is
                    really happening <q>physical</q>, and what is virtually happening
                    either <q>logical</q> or <q>abstract.</q>
                </p>
            </section>

            <section>
                <title>Isolation</title>

                <p>
                    One of the most important principles of engineering is isolation:
                    when you are designing a system with multiple components, it is usually
                    a good idea to isolate them from each other so that a change in one
                    component doesn't have undesired effects on other components.
                </p>

                <p>
                    One of the most important goals of an operating system is to isolate
                    each running program from the others so that programmers don't have to
                    think about every possible interaction.  The software object that
                    provides this isolation is a <term>process</term>.
                </p>

                <p>
                    A process is a software object that represents a running program.
                    I mean <q>software object</q> in the sense of object-oriented programming;
                    in general, an object contains data and provides methods
                    that operate on the data.  A process is an object that contains the
                    following data:<idx>process</idx>

                    <ul>
                        <li>
                            The text of the program, usually a sequence of
                            machine language instructions.
                        </li>
                        <li>
                            Data associated with the program, including static data (allocated
                            at compile time) and dynamic data (allocated at run time).
                        </li>
                        <li>
                            The state of any pending input/output operations.  For example,
                            if the process is waiting for data to be read from disk or for a
                            packet to arrive on a network, the status of these operations is
                            part of the process.
                        </li>
                        <li>
                            The hardware state of the program, which includes data stored
                            in registers, status information, and the program counter, which
                            indicates which instruction is currently executing.
                        </li>
                    </ul>
                </p>

                <p>
                    Usually one process runs one program, but it is also possible for
                    a process to load and run a new program.
                </p>
                <p>
                    It is also possible, and common, to run the same program in more than one
                    process.  In that case, the processes share the same program text
                    but generally have different data and hardware states.
                </p>
                <p>
                    Most operating systems provide a fundamental set of capabilities
                    to isolate processes from each other:
                    <dl width="narrow">
                        <li>
                            <title>Multitasking</title>
                            <p>
                                Most operating systems have the ability to
                                interrupt a running process at almost any time, save its hardware
                                state, and then resume the process later.  In general, programmers
                                don't have to think about these interruptions.  The program behaves
                                as if it is running continuously on a dedicated processor, 
                                except that the time between instructions is unpredictable.
                            </p>
                        </li>
                        <li>
                            <title>Virtual memory</title>
                            <p>
                                Most operating systems create the
                                illusion that each process has its own chunk of memory, isolated
                                from all other processes.  Again, programmers generally don't
                                have to think about how virtual memory works; they can proceed
                                as if every program has a dedicated chunk of memory.
                            </p>
                        </li>
                        <li>
                            <title>Device abstraction</title>
                            <p>
                                Processes running on the same computer share
                                the disk drive, the network interface, the graphics card, and other
                                hardware.  If processes interacted with this hardware directly,
                                without coordination, chaos would ensue.  For example, network data
                                intended for one process might be read by another.  Or multiple
                                processes might try to store data in the same location on a hard
                                drive.  It is up to the operating system to maintain order by
                                providing appropriate abstractions.
                            </p>
                        </li>
                    </dl>
                </p>

                <p>
                    As a programmer, you don't need to know much about how these
                    capabilities are implemented.  But if you are
                    curious, you will find a lot of interesting things
                    going on under the metaphorical hood.  And if you know what's
                    going on, it can make you a better programmer.
                 </p>
             </section>

             <section xml:id="unixps">
                 <title>UNIX processes</title>

                 <p>
                     While I write this book, the process I
                     am most aware of is my text editor, emacs.  Every once in a while
                     I switch to a terminal window, which is a window running a UNIX shell
                     that provides a command-line interface.
                 </p>
                 <p>
                     When I move the mouse, the window manager wakes up, sees that the
                     mouse is over the terminal window, and wakes up the terminal.
                     The terminal wakes up the shell.
                     If I type <c>make</c> in the shell, it creates a
                     new process to run Make, which creates another process to run LaTeX
                     and then another process to display the results.
                 </p>
                 <p>
                     If I need to look something up, I might switch to another desktop,
                     which wakes up the window manager again.  If I click on the icon for a
                     web browser, the window manager creates a process to run the web
                     browser.  Some browsers, like Chrome, create a new process for each
                     window and each tab.
                 </p>
                 <p>
                     And those are just the processes I am aware of.  At the same time
                     there are many other processes running in the <term>background}</term>.
                     Many of them are performing operations related to the operating
                     system.
                 </p>

                 <p>
                     The UNIX command <c>ps</c> prints information about running processes.
                     If you run it in a terminal, you might see something like this:
                 </p>
                 <console><output>
  PID TTY          TIME CMD
 2687 pts/1    00:00:00 bash
 2801 pts/1    00:01:24 emacs
24762 pts/1    00:00:00 ps
                 </output></console>
                 <p>
                     The first column is the unique numerical process ID.  The second
                     column is the terminal that created the process; 
                     <q>TTY</q> stands for teletypewriter, which was the original
                     mechanical terminal.
                 </p>
                 <p>
                     The third column is the total processor time used by the process,
                     in hours, minutes, and seconds.
                     The last column is the name of the running program.  In
                     this example, <c>bash</c> is the name of the shell that interprets
                     the commands I type in the terminal, emacs is my text editor, and
                     ps is the program generating this output.
                 </p>
                 <p>
                     By default, <c>ps</c> lists only the processes associated with
                     the current terminal.  If you use the <c>-e</c> flag, you get every
                     process (including processes belonging to other users, which is
                     a security flaw, in my opinion).
                 </p>
                 <p>
                     On my system there are currently 233 processes.
                     Here are some of them:
                 </p>
                 <console><output>
  PID TTY          TIME CMD
    1 ?        00:00:17 init
    2 ?        00:00:00 kthreadd
    3 ?        00:00:02 ksoftirqd/0
    4 ?        00:00:00 kworker/0:0
    8 ?        00:00:00 migration/0
    9 ?        00:00:00 rcu_bh
   10 ?        00:00:16 rcu_sched
   47 ?        00:00:00 cpuset
   48 ?        00:00:00 khelper
   49 ?        00:00:00 kdevtmpfs
   50 ?        00:00:00 netns
   51 ?        00:00:00 bdi-default
   52 ?        00:00:00 kintegrityd
   53 ?        00:00:00 kblockd
   54 ?        00:00:00 ata_sff
   55 ?        00:00:00 khubd
   56 ?        00:00:00 md
   57 ?        00:00:00 devfreq_wq
                 </output></console>
                 <p>
                     <c>init</c> is the first process created when the operating system
                     starts.  It creates many of the other processes, and then sits idle
                     until the processes it created are done.
                 </p>
                 <p>
                     <c>kthreadd</c> is a process the operating system uses to create new
                     <term>threads</term>.  We'll talk more about threads later, but for now you can
                     think of a thread as kind of a process.  The <c>k</c> at the beginning
                     stands for <term>kernel</term>, which is the part of the operating system
                     responsible for core capabilities like creating threads.  The extra
                     <c>d</c> at the end stands for <c>daemon</c>, which is another name for
                     processes like this that run in the background and provide operating
                     system services.  In this context, <q>daemon</q> is used in the
                     sense of a helpful spirit, with no connotation of evil.
                 </p>
                 <p>
                     Based on the name, you can infer that <c>ksoftirqd</c> is also a kernel
                     daemon; specifically, it handles software interrupt requests, or
                     <q>soft IRQ</q>.
                 </p>
                 <p>
                     <c>kworker</c> is a worker process created by the kernel to do some
                     kind of processing for the kernel.
                 </p>
                 <p>
                     There are often multiple processes running these kernel services.
                     On my system at the moment, there are 8 <c>ksoftirqd</c> processes
                     and 35 <c>kworker</c> processes.
                 </p>
                 <p>
                     I won't go into more details about the other processes, but if you
                     are interested you can search for more information about them.
                     You should run <c>ps</c> on your system and compare your results
                     to mine.
                 </p>
                 <!-- %TODO: using gdb here? -->
             </section>
        </chapter>

        <chapter xml:id="virtualmemory">
            <title>Virtual memory</title>

            <section>
                <title>A bit of information theory</title>

                <p>
                    A <term>bit</term> is a binary digit; it is also a unit of information.  If you
                    have one bit, you can specify one of two possibilities, usually
                    written 0 and 1.  If you have two bits, there are 4 possible
                    combinations, 00, 01, 10, and 11.  In general, if you have <m>b</m> bits, you
                    can indicate one of <m>2^b</m> values.  A <term>byte</term> is 8 bits, so it can
                    hold one of 256 values.
                    <idx>bit</idx>
                    <idx>byte</idx>
                </p>
                <p>
                    Going in the other direction, suppose you want to store a letter
                    of the alphabet.  There are 26 letters, so how many bits do you
                    need?  With 4 bits, you can specify one of 16 values, so that's
                    not enough.  With 5 bits, you can specify up to 32 values, so
                    that's enough for all the letters, with a few values left over.
                </p>
                <p>
                    In general, if you want to specify one of <m>N</m> values, you should
                    choose the smallest value of <m>b</m> so that <m>2^b \ge N</m>.  Taking the
                    log base 2 of both sides yields <m>b \ge log_2 N</m>.
                </p>
                <p>
                    Suppose I flip a coin and tell you the outcome.  I have given
                    you one bit of information.  If I roll a six-sided die and tell
                    you the outcome, I have given you <m>log_2 6</m> bits of information.
                    And in general, if the probability of the outcome is 1 in <m>N</m>,
                    then the outcome contains <m>log_2 N</m> bits of information.
                </p>
                <p>
                    Equivalently, if the probability of the outcome is <m>p</m>, then
                    the information content is <m>-log_2 p</m>.  This quantity is called
                    the <term>self-information</term> of the outcome.  It measures
                    how surprising the outcome is, which is why it is also called
                    <term>surprisal</term>.  If your horse has only one chance in 16 of winning,
                    and he wins, you get 4<nbsp/>bits of information (along with the
                    payout).  But if the favorite wins 75% of the time, the news
                    of the win contains only 0.42 bits.
                    <idx>self-information</idx>
                    <idx>surprisal</idx>
                </p>
                <p>
                    Intuitively, unexpected news carries a lot of
                    information; conversely, if there is something you were already confident
                    of, confirming it contributes only a small amount of information.
                </p>
                <p>
                    For several topics in this book, we will need to be comfortable
                    converting back and forth between the number of bits, <m>b</m>, and the
                    number of values they can encode, <m>N = 2^b</m>.
                </p>

            </section>
            <section>
                <title>Memory and storage</title>

                <p>
                    While a process is running, most of its data is held in
                    <term>main memory</term>, which is usually some kind of random access memory (RAM).
                    On most current computers, main memory is <term>volatile</term>, which means that
                    when the computer shuts down, the contents of main memory are lost.
                    A typical desktop computer has 2<ndash/>8 GiB of
                    memory.  GiB stands for <q>gibibyte,</q> which is <m>2^{30}</m> bytes.
                    <idx>main memory</idx>
                    <idx>volatile</idx>
                    <idx>gibibyte</idx>
                </p>
                <p>
                    If the process reads and writes files, those files are usually stored
                    on a hard disk drive (HDD) or solid state drive (SSD).  These storage
                    devices are <term>non-volatile</term>, so they are used for long-term storage.
                    Currently a typical desktop computer has a HDD with a capacity of
                    500 GB to 2 TB.  GB stands for <q>gigabyte,</q> which is <m>10^9</m> bytes.
                    TB stands for <q>terabyte,</q> which is <m>10^{12}</m> bytes.
                    <idx>gigabyte</idx>
                    <idx>terabyte</idx>
                </p>
                <p>
                    You might have noticed that I used the binary unit
                    GiB for the size of main memory and the decimal units GB and TB for
                    the size of the HDD.  For historical and technical reasons, memory is
                    measured in binary units, and disk drives are measured in decimal
                    units.  In this book I will be careful to distinguish binary and
                    decimal units, but you should be aware that the word <q>gigabyte</q> and the
                    abbreviation GB are often used ambiguously.
                </p>
                <p>
                    In casual use, the term <q>memory</q> is sometimes used for HDDs and SSDs
                    as well as RAM, but the properties of these devices are very
                    different, so we will need to distinguish them.  I will use
                    <term>storage</term> to refer to HDDs and SSDs.
                    <idx>storage</idx>
                </p>
            </section>
            <section>
                <title>Address spaces</title>

                <p>
                    Each byte in main memory is specified by an integer <term>physical
                    address</term>.  The set of valid physical addresses is called the
                    physical <term>address space</term>.  It
                    usually runs from 0 to <m>N-1</m>, where <m>N</m> is
                    the size of main memory.  On a system with 1 GiB of physical memory,
                    the highest valid address is <m>2^{30}-1</m>, which is 1,073,741,823 in
                    decimal, or 0x3fff<nbsp/>ffff in hexadecimal (the prefix 0x indicates a
                    hexadecimal number).
                    <idx>physical address</idx>
                    <idx>address space</idx>
                </p>
                <p>
                    However, most operating systems provide <term>virtual memory</term>, which
                    means that programs never deal with physical addresses, and don't 
                    have to know how much physical memory is available.
                    <idx>virtual memory</idx>
                </p>
                <p>
                    Instead, programs work with <term>virtual addresses</term>, which are numbered
                    from 0 to <m>M-1</m>, where <m>M</m> is the number of valid virtual addresses.
                    The size of the virtual address space is determined by the operating
                    system and the hardware it runs on.
                    <idx>virtual address</idx>
                </p>
                <p>
                    You have probably heard people talk about 32-bit and 64-bit systems.
                    These terms indicate the size of the registers, which is usually also
                    the size of a virtual address.  On a 32-bit system, virtual addresses
                    are 32<nbsp/>bits, which means that the virtual address space runs from 0 to
                    0xffff<nbsp/>ffff.  The size of this address space is <m>2^{32}</m> bytes, or
                    4<nbsp/> GiB.
                </p>
                <p>
                    On a 64-bit system, the size of the virtual address space is <m>2^{64}</m>
                    bytes, or <m>2^4 \cdot 1024^6</m> bytes.  That's 16<nbsp/>exbibytes, which is
                    about a billion times bigger than current physical memories.  It might
                    seem strange that a virtual address space can be so much bigger
                    than physical memory, but we will see soon how that works.
                </p>
                <p>
                    When a program reads and writes values in memory, it generates virtual
                    addresses.  The hardware, with help from the operating system,
                    translates to physical addresses before accessing main memory.  This
                    translation is done on a per-process basis, so even if two processes
                    generate the same virtual address, they would map to different
                    locations in physical memory.
                </p>
                <p>
                    Thus, virtual memory is one important way the operating system
                    isolates processes from each other.  In general, a process cannot
                    access data belonging to another process, because there is no
                    virtual address it can generate that maps to physical memory
                    allocated to another process.
                </p>
            </section>
            <section>
                <title>Memory segments</title>

                <p>
                    The data of a running process is organized into five segments:
                    <ul>
                        <li>
                            The <term>code segment</term> contains the program text; that is, the
                            machine language instructions that make up the program.
                            <idx>code segment</idx>
                        </li>
                        <li>
                            The <term>static segment</term> contains immutable values, like string
                            literals.  For example, if your program contains the string
                            <c>"Hello, World"</c>, those characters will be stored in the
                            static segment.
                            <idx>static segment</idx>
                        </li>

                        <li>
                            The <term>global segment</term> contains global variables and local variables
                            that are declared <c>static</c>.
                            <idx>global segment</idx>
                        </li>

                        <li>
                            The <term>heap segment</term> contains chunks of memory allocated
                            at run time, most often by calling the C library function
                            <c>malloc</c>.
                            <idx>heap segment</idx>
                        </li>

                        <li>
                            The <term>stack</term> contains the call stack, which is a
                            sequence of stack frames.  Each time a function is called, a stack
                            frame is allocated to contain the 
                            parameters and local variables of the function.  When the function
                            completes, its stack frame is removed from the stack.
                        </li>
                    </ul>
                </p>
                <p>
                    The arrangement of these segments is determined partly by the 
                    compiler and partly by the operating system.  The details vary
                    from one system to another, but in the most common arrangement:

                    <ol>
                        <li>
                            The text segment is near the <q>bottom</q> of memory, that is,
                            at addresses near 0.
                        </li>
                        <li>
                            The static segment is often just above the text segment, that is,
                            at higher addresses.
                        </li>
                        <li>
                            The global segment is often just above the static segment.
                        </li>
                        <li>
                            The heap is often above the global segment.  As it expands,
                            it grows up toward larger addresses.
                        </li>
                        <li>
                            The stack is near the top of memory; that is, near the
                            highest addresses in the virtual address space.  As the
                            stack expands, it grows down toward smaller addresses.
                        </li>
                    </ol>
                </p>

                
                <!-- %TODO: Figure out how to handle the code that is in both ExercisesInC
                     % and the repo for the book.
                -->

                <p>
                    To determine the layout of these segments on your system, try running
                    this program, which is in <c>aspace.c</c> in the repository for this
                    book (see <xref ref="code"/>).
                </p>

                <listing>
                    <title><c>aspace.c</c></title>
                    <program language="cpp"><code><![CDATA[
#include <stdio.h>
#include <stdlib.h>

int global;

int main ()
{
    int local = 5;
    void *p = malloc(128);
    char *s = "Hello, World";

    printf ("Address of main is %p\n", main);
    printf ("Address of global is %p\n", &global);
    printf ("Address of local is %p\n", &local);
    printf ("p points to %p\n", p);
    printf ("s points to %p\n", s);
}
]]>
                    </code></program>
                </listing>

                <p>
                    <c>main</c> is the name of a function; when it is used as a variable,
                    it refers to the address of the first machine language instruction
                    in <c>mai</c>, which we expect to be in the text segment.
                </p>
                <p>
                    <c>global</c> is a global variable, so we expect it to be in the
                    global segment.  <c>local</c> is a local variable, so we expect it
                    to be on the stack.
                </p>
                <p>
                    <c>s</c> refers to a <q>string literal</q>, which is a string that appears
                    as part of the program (as opposed to a string that is read from a file,
                    input by a user, etc.).  We expect the location of the string to be
                    in the static segment (as opposed to the pointer, <c>s</c>, which is
                    a local variable).
                </p>
                <p>
                    <c>p</c> contains an address returned by <c>malloc</c>, which allocates
                    space in the heap.  <q>malloc</q> stands for <q>memory allocate.</q>
                </p>
                <p>
                    The format sequence <c>"%p"</c> tells <c>printf</c> to format each
                    address as a <q>pointer</q>, so it displays the results in hexadecimal.
                </p>
                <p>
                    When I run this program, the output looks like this (I added spaces
                    to make it easier to read):
                </p>
                <console><output>
Address of main is   0x      40057d
Address of global is 0x      60104c
Address of local is  0x7ffe6085443c
p points to          0x     16c3010
s points to          0x      4006a4
                </output></console>
                <p>
                    As expected, the address of <c>main</c> is the lowest, followed by
                    the location of the string literal.  The location of
                    <c>global</c> is next, then the address <c>p</c> points to.
                    The address of <c>local</c> is much bigger.
                </p>
                <p>
                    The largest address has 12 hexadecimal digits.  Each hex digit
                    corresponds to 4 bits, so it is a 48-bit address.  That suggests
                    that the usable part of the virtual address space is <m>2^{48}</m> bytes.
                </p>
                <p>
                    As an exercise, run this program on your computer and compare your
                    results to mine.  Add a second call to <c>malloc</c> and check whether
                    the heap on your system grows up (toward larger addresses).  Add a
                    function that prints the address of a local variable, and check
                    whether the stack grows down.
                </p>
            </section>

            <section>
                <title>Static local variables</title>
                <p>
                    Local variables on the stack are sometimes called <term>automatic</term>,
                    because they are allocated automatically when a function is called,
                    and freed automatically when the function returns.
                    <idx>automatic</idx>
                </p>
                <p>
                    In C there is another kind of local variable, called <c>static</c>,
                    which is allocated in the global segment.  It is initialized when
                    the program starts and keeps its value from one function call to
                    the next.
                    <idx>static</idx>
                </p>
                <p>
                    For example, the following function keeps track of how many times
                    it has been called.
                </p>
                <program language="cpp"><code><![CDATA[
int times_called()
{
    static int counter = 0;
    counter++;
    return counter;
}
    ]]>
                </code></program>
                <p>
                    The keyword <c>static</c> indicates that <c>counter</c> is a static
                    local variable.  The initialization happens only once, when the program
                    starts.
                </p>
                <p>
                    If you add this function to <c>aspace.c</c> you can confirm that
                    <c>counter</c> is allocated in the global segment along with global
                    variables, not in the stack.
                </p>
            </section>
            <section xml:id="address_translation">
                <title>Address translation</title>
                <p>
                    How does a virtual address (VA) get translated to a physical address (PA)?
                    The basic mechanism is simple, but a simple
                    implementation would be too slow and take too much space.  So actual
                    implementations are a bit more complicated.
                </p>

                <figure xml:id="addtrans">
                    <caption>Address translation</caption>
                    <image source="address_translation.png" />
                </figure>

                <p>
                    Most processors provide a memory management unit (MMU) that sits between the
                    CPU and main memory.  The MMU performs fast translation between VAs and PAs.
                </p>
                <p><ol>
                    <li>
                        When a program reads or writes a variable, the CPU generates a VA.
                    </li>
                    <li>
                        The MMU splits the VA into two parts, called the page number and
                        the offset.  A <q>page</q> is a chunk of memory; the size of a page
                        depends on the operating system and the hardware, but common sizes
                        are 1<ndash/>8 KiB.
                    </li>
                    <li>
                        The MMU looks up the page number in the translation lookaside buffer
                        (TLB) and gets the corresponding physical page number.  Then it combines
                        the physical page number with the offset to produce a PA.
                    </li>
                    <li>
                        The PA is passed to main memory, which reads or writes the given location.
                    </li>
                </ol></p>

                <p>
                    The TLB contains cached copies of data from the page table (which is stored in
                    kernel memory).  The page table contains the mapping from virtual page numbers
                    to physical page numbers.  Since each process has its own page table, the TLB
                    has to make sure it only uses entries from the page table of the process
                    that's running.
                </p>

                <p>
                    <xref ref="addtrans"/> shows a diagram of this process.
                    To see how it all works, suppose that the VA is 32<nbsp/>bits and the physical memory
                    is 1<nbsp/>GiB, divided into 1<nbsp/>KiB pages.
                </p>

                <p><ul>
                    <li>
                        Since 1<nbsp/>GiB is <m>2^{30}</m> bytes and 1<nbsp/>KiB is <m>2^{10}</m> bytes, there
                        are <m>2^{20}</m> physical pages, sometimes called <q>frames.</q>
                    </li>
                    <li>
                        The size of the virtual address space is <m>2^{32}</m><nbsp/>B and the size
                        of a page is <m>2^{10}</m><nbsp/>B, so there are <m>2^{22}</m> virtual pages.
                    </li>
                    <li>
                        The size of the offset is determined by the page size.  In this
                        example the page size is <m>2^{10}</m><nbsp/>B, so it takes 10<nbsp/>bits to specify
                        a byte on a page.
                    </li>
                    <li>
                        If a VA is 32<nbsp/>bits and the offset is 10<nbsp/>bits, the remaining
                        22<nbsp/>bits make up the virtual page number.
                    </li>
                    <li>
                        Since there are <m>2^{20}</m> physical pages, each physical page
                        number is 20<nbsp/>bits.  Adding in the 10<nbsp/>bit offset, the resulting
                        PAs are 30<nbsp/>bits.
                    </li>
                </ul></p>

                <p>
                    So far this all seems feasible.  But let's think about how big a page
                    table might have to be.  The simplest implementation of a page
                    table is an array with one entry for each virtual page.
                    Each entry would contain a physical page number, which is 20 bits
                    in this example, plus some additional information about each
                    frame.  So we expect 3<ndash/>4 bytes per entry.  But with <m>2^{22}</m> virtual pages,
                    the page table would require <m>2^{24}</m> bytes, or 16<nbsp/>MiB.
                </p>
                <p>
                    And since we need a page table for each process, a system running
                    256 processes would need <m>2^{32}</m> bytes, or 4<nbsp/>GiB, just for page tables!
                    And that's just with 32-bit virtual addresses.  With 48- or 64-bit
                    VAs, the numbers are ridiculous.
                </p>
                <p>
                    Fortunately, we don't actually need that much space, because
                    most processes don't use even a small fraction of their
                    virtual address space.  And if a process doesn't use a virtual
                    page, we don't need an entry in the page table for it.
                </p>
                <p>
                    Another way to say the same thing is that page tables are <q>sparse</q>,
                    which implies that the simple implementation, an array of page
                    table entries, is a bad idea.  Fortunately, there are several
                    good implementations for sparse arrays.
                </p>
                <p>
                    One option is a multilevel page table, which is what many operating
                    systems, including Linux, use.  Another option is an associative table,
                    where each entry includes both the virtual page number and the physical
                    page number.  Searching an associative table can be slow in software,
                    but in hardware we can search the entire table in parallel, so associative arrays are
                    often used to represent the page table entries in the TLB.
                </p>
                <p>
                    You can read more about these implementations on
                    <url href="https://en.wikipedia.org/wiki/Page_table">Wikipedia</url>; you might find the
                    details interesting.  But the fundamental idea is that page tables are
                    sparse, so we have to choose a good implementation for sparse arrays.
                </p>
                <p>
                    I mentioned earlier that the operating system can interrupt a running
                    process, save its state, and then run another process.  This mechanism
                    is called a <term>context switch</term>.  Since each process has its own
                    page table, the operating system has to work with the MMU to make
                    sure each process gets the right page table.  In older machines,
                    the page table information in the MMU had to be replaced during every
                    context switch, which was expensive.  In newer systems, each page
                    table entry in the MMU includes the process ID, so page tables from
                    multiple processes can be in the MMU at the same time.
                </p>
            </section>
        </chapter>

        <chapter xml:id="filesystems">
            <title>Files and file systems</title>

            <introduction>
                <p>
                    When a process completes (or crashes), any data stored in main
                    memory is lost.  But data stored on a hard disk drive (HDD) or
                    solid state drive (SSD) is <term>persistent</term>; that is, it survives
                    after the process completes, even if the computer shuts down.
                    <idx>persistent</idx>
                </p>
                <p>
                    Hard disk drives are complicated.  Data is stored in blocks, which
                    are laid out in sectors, which make up tracks, which are arranged
                    in concentric circles on platters.
                    <idx>block</idx>
                    <idx>sector</idx>
                    <idx>track</idx>
                </p>
                <p>
                    Solid state drives are simpler in one sense, because blocks are
                    numbered sequentially, but they raise a different complication: each
                    block can be written a limited number of times before it becomes
                    unreliable.
                </p>
                <p>
                    As a programmer, you don't want to deal with these complications.
                    What you want is an appropriate abstraction of persistent storage
                    hardware.  The most common abstraction is called a <term>file system.</term>
                </p>
                <p>
                    Abstractly:
                    <ul>
                        <li>
                            A <term>file system</term> is a mapping from each file's name to its contents.
                            If you think of the names as keys, and the contents as values,
                            a file system is a kind of key-value database
                            (see <url href="https://en.wikipedia.org/wiki/Key-value_database">Wikipedia</url>).
                            <idx>file system</idx>
                        </li>
                        <li>
                            A <term>file</term> is a sequence of bytes.
                            <idx>file</idx>
                        </li>
                    </ul>
                </p>
                <p>
                    File names are usually strings, and they are usually <q>hierarchical</q>;
                    that is, the string specifies a path from a top-level directory (or
                    folder), through a series of subdirectories, to a specific file.
                </p>
                <p>
                    The primary difference between the abstraction and the underlying
                    mechanism is that files are byte-based and persistent storage is
                    block-based.  The operating system translates byte-based file operations 
                    in the C library into block-based operations on storage devices.
                    Typical block sizes are 1<ndash/>8<nbsp/>KiB.
                </p>
                <p>
                    For example, the following code opens a file and reads the first byte:
                </p>
                <program language="cpp"><code><![CDATA[
                        FILE *fp = fopen("/home/downey/file.txt", "r");
                        char c = fgetc(fp);
                        fclose(fp);
                ]]></code></program>
                <p>
                    When this code runs:
                    <ol>
                        <li>
                            <c>fopen</c> uses the filename to find the top-level directory,
                            called <c>"/"</c>, the subdirectory <c>"home"</c>, and the
                            sub-subdirectory <c>"downey"</c>.
                        </li>
                        <li>
                            It finds the file named <c>file.txt</c> and <q>opens</q> it for
                            reading, which means it creates a data structure that represents the
                            file being read.  Among other things, this data structure
                            keeps track of how much of the file has been read, called the
                            <term>file position</term>.
                        </li>
                        <li>
                            In DOS, this data structure is called a File Control Block, but I
                            want to avoid that term because in UNIX it means something else.  In
                            UNIX, there seems to be no good name for it.  It is an entry in the
                            open file table, so I will call it an <c>OpenFileTableEntry</c>.
                        </li>
                        <li>
                            When we call <c>fgetc</c>, the operating system checks whether
                            the next character of the file is already in memory.  If so, it
                            reads the next character, advances the file position, and returns
                            the result.
                        </li>
                        <li>
                            If the next character is not in memory, the operating
                            system issues an I/O request to get the next block.  Disk drives are
                            slow, so a process waiting for a block from disk is usually
                            interrupted so another process can run until the data arrives.
                        </li>
                        <li>
                            When the I/O operation is complete, the new block of data is
                            stored in memory, and the process resumes.  It reads the first
                            character and stores it as a local variable.
                        </li>
                        <li>
                            When the process closes the file, the operating system completes
                            or cancels any pending operations, removes data stored in
                            memory, and frees the <c>OpenFileTableEntry</c>.
                        </li>
                    </ol>
                </p>
                <p>
                    The process for writing a file is similar, but there are some
                    additional steps.  Here is an example that opens a file for
                    writing and changes the first character.
                </p>
                <program language="cpp"><code>
                        FILE *fp = fopen("/home/downey/file.txt", "w");
                        fputc('b', fp);
                        fclose(fp);
                </code></program>
                <p>
                    When this code runs:
                    <ol>
                        <li>
                            Again, <c>fopen</c> uses the filename to find the file.  If it
                            does not already exist, it creates a new file and adds an entry in
                            the parent directory, <c>/home/downey</c>.
                        </li>
                        <li>
                            The operating system creates an <c>OpenFileTableEntry</c> that
                            indicates that the file is open for writing, and sets the file
                            position to 0.
                        </li>
                        <li>
                            <c>fputc</c> attempts to write (or re-write) the first byte of
                            the file.  If the file already exists, the operating system has to
                            load the first block into memory.  Otherwise it allocates a new
                            block in memory and requests a new block on disk.
                        </li>
                        <li>
                            After the block in memory is modified, it might not be copied
                            back to the disk right away.  In general, data written to a file is
                            <term>buffered</term>, which means it is stored in memory and only written to
                            disk when there is at least one block to write.
                            <idx>buffered</idx>
                        </li>
                        <li>
                            When the file is closed, any buffered data is written to disk
                            and the <c>OpenFileTableEntry</c> is freed.
                        </li>
                    </ol>
                </p>
                <p>
                    To summarize, the C library provides the abstraction of a file
                    system that maps from file names to streams of bytes.  This abstraction
                    is built on top of storage devices that are actually organized
                    in blocks.
                </p>
            </introduction>
            <section>
                <title>Disk performance</title>

                <p>
                    I mentioned earlier that disk drives are slow.  On current HDDs, the
                    average time to read a block from disk to memory might be 5<ndash/>25<nbsp/>ms
                    (see <url href="https://en.wikipedia.org/wiki/Hard_disk_drive_performance_characteristics">Wikipedia</url>).
                    SSDs are faster, taking 25<nbsp/>&#956;s to read a 4<nbsp/>KiB block and 250<nbsp/>&#956;s to
                    write one (see <url href="https://en.wikipedia.org/wiki/Ssd#Controller">Wikipedia</url>).
                </p>
                <p>
                    To put these numbers in perspective, let's compare them to the clock
                    cycle of the CPU.  A processor with clock rate 2<nbsp/>GHz completes one
                    clock cycle every 0.5<nbsp/>ns.  The time to get a byte from memory to
                    the CPU is typically around 100<nbsp/>ns.  If the processor completes one
                    instruction per clock cycle, it would complete 200 instructions
                    while waiting for a byte from memory.
                </p>
                <p>
                    In one microsecond, it would complete 2000 instructions,
                    so while waiting 25<nbsp/>&#956;s for a byte from an SSD, it would complete 50,000.
                </p>
                <p>
                    In one millisecond, it would complete 2,000,000 instructions,
                    so while waiting 20<nbsp/>ms for a byte from a HDD, it might complete
                    40 million.  If there's nothing for the CPU to do while it waits,
                    it would be idle.  That's why the operating system generally
                    switches to another process while it is waiting for data from disk.
                </p>
                <p>
                    The gap in performance between main memory and persistent storage is
                    one of the major challenges of computer system design.  Operating
                    systems and hardware provide several features intended to <q>fill in</q>
                    this gap:

                    <ul>
                        <li>
                            <term>Block transfers</term>: The time it takes to load a single byte from
                            disk is 5<ndash/>25<nbsp/>ms.  By comparison, the additional time to load an
                            8<nbsp/>KiB block is negligible.  So systems generally try to read large
                            blocks each time they access the disk.
                        </li>
                        <li>
                            <term>Prefetching</term>: Sometimes the operating system can predict that a
                            process will read a block and start loading it before it is
                            requested.  For example, if you open a file and read the first
                            block, there is a good chance you will go on to read the second
                            block.  The operating system might start loading additional blocks
                            before they are requested.
                        </li>
                        <li>
                            <term>Buffering</term>: As I mentioned, when you write a file, the operating
                            system stores the data in memory and only writes it to disk later.
                            If you modify the block several times while it is in memory, the
                            system only has to write it to disk once.
                        </li>
                        <li>
                            <term>Caching</term>: If a process has used a block recently, it is likely to
                            use it again soon.  If the operating system keeps a copy of the
                            block in memory, it can handle future requests at memory speed.
                        </li>
                    </ul>
                </p>
                <p>
                    Some of these features are also implemented in hardware.  For example,
                    some disk drives provide a cache that stores recently-used blocks,
                    and many disk drives read more than one block at a time, even if only
                    one is requested.
                </p>
                <p>
                    These mechanisms generally improve the performance of
                    programs, but they don't change the behavior.  Usually programmers
                    don't have to think about them, with two exceptions: (1) if the
                    performance of a program is unexpectedly bad, you might have to know
                    something about these mechanisms to diagnose the problem, and (2)
                    when data is buffered, it can be harder to debug a program.  For
                    example, if a program prints a value and then crashes, the value
                    might not appear, because it might be in a buffer.  Similarly, if a
                    program writes data to disk and then the computer loses power, the
                    data might be lost if it is in a cache and not yet on disk.
                </p>
            </section>

            <section>
                <title>Disk metadata</title>

                <p>
                    The blocks that make up a file might be arranged contiguously on
                    disk, and file system performance is generally better if they are,
                    but most operating systems don't require contiguous allocation.
                    They are free to place a block anywhere on disk, and they use
                    various data structures to keep track of them.
                </p>
                <p>
                    In many UNIX file systems, that data structure is called an <term>inode</term>,
                    which stands for <q>index node</q>.  More generally, information about
                    files, including the location of their blocks, is called <term>metadata</term>.
                    (The content of the file is data, so information about the file is
                    data about data, hence <q>meta</q>.)
                    <idx>metadata</idx>
                    <idx>inode</idx>
                </p>
                <p>
                    Since inodes reside on disk along with the rest of the data, they are
                    designed to fit neatly into disk blocks.  A UNIX inode contains
                    information about a file, including the user ID of the file owner;
                    permission flags indicating who is allowed to read, write, or execute
                    it; and timestamps that indicate when it was last modified and
                    accessed.  In addition, it contains block numbers for the first 12
                    blocks that make up the file.
                </p>
                <p>
                    If the block size is 8 KiB, the first 12 blocks make up 96 KiB.
                    On most systems, that's big enough for a large majority of files,
                    but it's definitely not big enough for all of them.  That's
                    why the inode also contains a pointer to an <term>indirection block</term>,
                    which contains nothing but pointers to other blocks.
                    <idx>indirection block</idx>
                </p>
                <p>
                    The number of pointers in an indirection block depends on the sizes of
                    the blocks and the block numbers, but it is often 1024.  With 1024
                    block numbers and 8 KiB blocks, an indirection block can address 8
                    MiB.  That's big enough for all but the largest files, but still not
                    big enough for all.
                </p>

                <p>
                    That's why the inode also contains a pointer to a
                    <term>double indirection block</term>, which contains pointers to
                    indirection blocks.  With 1024 indirection blocks, we can address 8<nbsp/>GiB.
                    <idx>double indirection block</idx>
                </p>
                <p>
                    And if that's not big enough, there is (finally) a
                    <term>triple indirection block</term>,
                    which contains pointers to double indirection blocks, yielding
                    a maximum file size of 8 TiB.  When UNIX inodes were designed, that
                    seemed big enough to serve for a long time.  But that was a long time ago.
                </p>
                <p>
                    As an alternative to indirection blocks, some files systems, like FAT,
                    use a <q>File Allocation Table</q> that contains one entry for each block,
                    called a <term>cluster</term> in this context.  A root directory contains a
                    pointer to the first cluster in each file.  The FAT entry for each
                    cluster points to the next cluster in the file, similar to a linked
                    list.  For more details, see
                    <url href="https://en.wikipedia.org/wiki/File_Allocation_Table">File Allocation Table</url>.
                    <idx>cluster</idx>
                </p>
            </section>
            <section>
                <title>Block allocation</title>

                <p>
                    File systems have to keep track of which blocks belong to each file;
                    they also have to keep track of which blocks are available for use.
                    When a new file is created, the file system finds an available
                    block and allocates it.  When a file is deleted, the file system
                    makes its blocks available for re-allocation.
                </p>
                <p>
                    The goals of the block allocation system are:

                    <ul>
                        <li>
                            <title>Speed</title>
                            <p>Allocating and freeing blocks should be fast.</p>
                        </li>
                        <li>
                            <title>Minimal space overhead</title>
                            <p>
                                The data structures used by the allocator
                                should be small, leaving as much space as possible for data.
                            </p>
                        </li>
                        <li>
                            <title>Minimal fragmentation</title>
                            <p>
                                If some blocks are left unused, or some
                                are only partially used, the unused space is called
                                <term>fragmentation</term>.
                                <idx>fragmentation</idx>
                            </p>
                        </li>
                        <li>
                            <title>Maximum contiguity</title>
                            <p>
                                Data that is likely to be used at the same time should
                                be physically contiguous, if possible, to improve performance.
                            </p>
                        </li>
                    </ul>
                </p>
                <p>
                    It is hard to design a file system that achieves all of these
                    goals, especially since file system performance depends on
                    <term>workload characteristics</term> like file sizes, access
                    patterns, etc.  A file system that is well tuned for one workload
                    might not perform as well for another.
                    <idx>workload characteristics</idx>
                </p>
                <p>
                    For this reason, most operating systems support several kinds of file
                    systems, and file system design is an active area of research and
                    development.  In the last decade, Linux systems have migrated
                    from <term>ext2</term>, which was a conventional UNIX file system,
                    to <term>ext3</term>,
                    a <term>journaling</term> file system intended to improve speed and
                    contiguity, and more recently to <term>ext4</term>, which can handle larger files
                    and file systems.  Within the next few years, there might be
                    another migration to the B-tree file system, Btrfs.
                </p>
            </section>
            <section>
                <title>Everything is a file?</title>

                <p>
                    The file abstraction is really a <q>stream of bytes</q> abstraction,
                    which turns out to be useful for many things, not just file systems.
                </p>
                <p>
                    One example is the UNIX pipe, which is a simple form of inter-process
                    communication.  Processes can be set up so that output from one
                    process is taken as input into another process.  For the first
                    process, the pipe behaves like a file open for writing, so it
                    can use C library functions like <c>fputs</c> and <c>fprintf</c>.
                    For the second process, the pipe behaves like a file open for
                    reading, so it uses <c>fgets</c> and <c>fscanf</c>.
                </p>
                <p>
                    Network communication also uses the stream of bytes abstraction.
                    A UNIX socket is a data structure that represents a communication
                    channel between processes on different computers (usually).  Again,
                    processes can read data from and write data to a socket using
                    <q>file</q> handling functions.
                </p>
                <p>
                    Reusing the file abstraction makes life easier for programmers, since
                    they only have to learn one API (application program interface).
                    It also makes programs more versatile, since a program intended to
                    work with files can also work with data coming from pipes and other
                    sources.
                </p>
                
                <!-- % TODO: gprof here? -->
            </section>
        </chapter>

        <chapter xml:id="bitsandbytes">
            <title>More bits and bytes</title>

            <section>
                <title>Representing integers</title>

                <p>
                    You probably know that computers represent numbers in
                    base 2, also known as binary.  For positive numbers, the binary
                    representation is straightforward; for example, the representation
                    for <m>5_{10}</m> is <m>101_2</m>.
                </p>
                <p>
                    For negative numbers, the most obvious representation uses
                    a sign bit to indicate whether a number is positive or negative.
                    But there is another representation, called <term>two's complement</term>
                    that is much more common because it is easier to work with
                    in hardware.
                    <idx>two's complement</idx>
                </p>
                <p>
                    To find the two's complement of a negative number, <m>-x</m>, find
                    the binary representation of <m>x</m>, flip all the bits, and add 1.
                    For example, to represent <m>-5_{10}</m>, start with the representation
                    of <m>5_{10}</m>, which is <m>0000.0101_2</m> if we write the 8-bit version.
                    Flipping all the bits and adding 1 yields <m>1111.1011_2</m>.
                </p>
                <p>
                    In two's complement, the leftmost bit acts like a sign bit;
                    it is 0 for positive numbers and 1 for negative numbers.
                </p>
                <p>
                    To convert from an 8-bit number to 16-bits, we have to add
                    more 0's for a positive number and add 1's for a negative number.
                    In effect, we have to copy the sign bit into the new bits.
                    This process is called <term>sign extension</term>.
                </p>
                <p>
                    In C all integer types are signed (able to represent positive and
                    negative numbers) unless you declare them <c>unsigned</c>.  The
                    difference, and the reason this declaration is important, is that
                    operations on unsigned integers don't use sign extension.
                </p>
            </section>
            <section>
                <title>Bitwise operators</title>

                <p>
                    People learning C are sometimes confused
                    about the bitwise operators <c>&amp;</c> and <c>|</c>.  These
                    operators treat integers as bit vectors and compute logical
                    operations on corresponding bits.
                </p>
                <p>
                    For example, <c>&amp;</c> computes the AND operation, which yields
                    1 if both operands are 1, and 0 otherwise.  Here is an example
                    of <c>&amp;</c> applied to two 4-bit numbers:
                </p>
                <console><output>
  1100
&amp; 1010
------
  1000
                </output></console>
                <p>
                    In C, this means that the expression <c>12 &amp; 10</c> has the
                    value 8.
                </p>
                <p>
                    Similarly, <c>|</c> computes the OR operation, which yields
                    1 if either operand is 1, and 0 otherwise.
                </p>
                <console><output>
                    1100
                  | 1010
                  ------
                    1110
                </output></console>
                <p>
                    So the expression <c>12 | 10</c> has the value 14.
                </p>
                <p>
                    Finally, <c>^</c> computes the XOR operation, which yields
                    1 if either operand is 1, but not both.
                </p>
                <console><output>
                    1100
                  ^ 1010
                  ------
                    0110
                </output></console>
                <p>
                    So the expression <c>12 ^ 10</c> has the value 6.
                </p>
                <p>
                    Most commonly, <c>&amp;</c> is used to clear a set of bits from
                    a bit vector, <c>|</c> is used to set bits, and <c>^</c>
                    is used to flip, or <q>toggle</q> bits.  Here are the details:
                </p>
                <paragraphs>
                    <title>Clearing bits</title>
                    <p>
                        For any value <m>x</m>, <m>x \&amp; 0</m> is 0, and <m>x \&amp; 1</m> is <m>x</m>.
                        So if you AND a vector with 3, it 
                        selects only the two rightmost bits, and sets the rest to 0.
                    </p>
                    <console><output>
                        xxxx
                      &amp; 0011
                      ------
                        00xx
                    </output></console>
                    <p>
                        In this context, the value 3 is called a <term>mask</term> because it
                        selects some bits and masks the rest.
                        <idx>term</idx>
                    </p>
                </paragraphs>
                <paragraphs>
                    <title>Setting bits</title>
                    <p>
                        Similarly, for any <m>x</m>, <m>x | 0</m> is x, and <m>x | 1</m> is <m>1</m>.
                        So if you OR a vector with 3, it sets the rightmost
                        bits, and leaves the rest alone:
                    </p>
                    <console><output>
                      xxxx
                    | 0011
                    ------
                      xx11
                    </output></console>
                </paragraphs>
                <paragraphs>
                    <title>Toggling bits</title>
                    <p>
                        Finally, if you XOR a vector with 3, it flips the
                        rightmost bits and leaves the rest alone.  As an exercise, see if you
                        can compute the two's complement of 12 using <c>^</c>.  Hint: what's
                        the two's complement representation of -1?
                    </p>
                </paragraphs>
                <!-- % (12 ^ -1) + 1 -->
                <p>
                    C also provides shift operators, <c>&lt;&lt;</c> and <c>&gt;&gt;</c>, which shift
                    bits left and right.  Each left shift doubles a number, so
                    <c>5 &lt;&lt; 1</c> is 10, and <c>5 &lt;&lt; 2</c> is 20.  Each right shift
                    divides by two (rounding down), so <c>5 &gt;&gt; 1</c> is 2 and
                    <c>2 &gt;&gt; 1</c> is 1.
                </p>
            </section>

            <section>
                <title>Representing floating-point numbers</title>

                <p>
                    Floating-point numbers are represented using the binary
                    version of scientific notation.  In decimal notation, large
                    numbers are written as the product of a coefficient and 10 raised
                    to an exponent.  For example, the speed of light in m/s is
                    approximately <m>2.998 \cdot 10^8</m>.
                </p>
                <p>
                    Most computers use the IEEE standard for floating-point
                    arithmetic.  The C type <c>float</c> usually corresponds
                    to the 32-bit IEEE standard; <c>double</c> usually corresponds
                    to the 64-bit standard.
                </p>
                <p>
                    In the 32-bit standard, the leftmost bit is the sign bit, <m>s</m>.
                    The next 8 bits are the exponent, <m>q</m>, and the last 23 bits are
                    the coefficient, <m>c</m>.  The value of a floating-point number is:
                    <me>
                        (-1)^s c \cdot 2^q
                    </me>
                </p>
                <p>
                    Well, that's almost correct, but there's one more wrinkle.
                    Floating-point numbers are usually normalized so that there is
                    one digit before the point.  For example, in base 10, we prefer
                    <m>2.998 \cdot 10^8</m> rather than <m>2998 \cdot 10^5</m> or any other
                    equivalent expression.  In base 2, a normalized number always
                    has the digit 1 before the binary point.  Since the digit in
                    this location is always 1, we can save space by leaving it
                    out of the representation.
                </p>
                <p>
                    For example, the integer representation of <m>13_{10}</m> is <m>1101_2</m>.
                    In floating point, that's <m>1.101 \cdot 2^3</m>, so the exponent
                    is 3 and the part of the coefficient that would be stored
                    is 101 (followed by 20 zeros).
                </p>
                <p>
                    Well, that's almost correct, but there's one more wrinkle.
                    The exponent is stored with a <term>bias</term>.  In the 32-bit standard,
                    the bias is 127, so the exponent 3 would be stored as 130.
                </p>
                <p>
                    To pack and unpack floating-point numbers in C, we can use a 
                    union and bitwise operations.  Here's an example:
                </p>
                <program language="cpp"><code><![CDATA[
                    union {
                        float f;
                        unsigned int u;
                    } p;

                    p.f = -13.0;
                    unsigned int sign = (p.u >> 31) & 1;
                    unsigned int exp = (p.u >> 23) & 0xff;

                    unsigned int coef_mask = (1 << 23) - 1;
                    unsigned int coef = p.u & coef_mask;

                    printf("%d\n", sign);
                    printf("%d\n", exp);
                    printf("0x%x\n", coef);
                ]]></code></program>
                <p>
                    This code is in <c>float.c</c> in the repository for this
                    book (see <xref ref="code" />).
                </p>
                <p>
                    The union allows us to store a floating-point value using
                    <c>p.f</c> and then read it as an unsigned integer using
                    <c>p.u</c>.
                </p>
                <p>
                    To get the sign bit, we shift the bits to the right 31
                    places and then use a 1-bit mask to select only the
                    rightmost bit.
                </p>
                <p>
                    To get the exponent, we shift the bits 23 places, then select the
                    rightmost 8 bits (the hexadecimal value <c>0xff</c> has eight ones).
                </p>
                <p>
                    To get the coefficient, we need to extract the 23 rightmost bits
                    and ignore the rest.  We do that by making a mask with 1s in the
                    23 rightmost places and 0s on the left.  The easiest way to do that
                    is by shifting 1 to the left by 23 places and then subtracting 1.  
                </p>
                <p>
                    The output of this program is:
                </p>
                <console><output>
                    1
                    130
                    0x500000
                </output></console>
                <p>
                    As expected, the sign bit for a negative number is 1.  The exponent 
                    is 130, including the bias.  And the coefficient, which I printed in
                    hexadecimal, is <m>101</m> followed by 20 zeros.
                </p>
                <p>
                    As an exercise, try assembling or disassembling a <c>double</c>, which
                    uses the 64-bit standard.
                    See <url href="https://en.wikipedia.org/wiki/IEEE_floating_point">IEEE floating point</url>.
                </p>
            </section>

            <section>
                <title>Unions and memory errors</title>

                <p>
                    There are two common uses of C unions.  One, which we saw in the
                    previous section, is to access the binary representation of data.
                    Another is to store heterogeneous data.  For example, you could
                    use a union to represent a number that might be an integer, float,
                    complex, or rational number.
                </p>
                <p>
                    However, unions are error-prone.  It is up to you, as the programmer,
                    to keep track of what type of data is in the union; if you write
                    a floating-point value and then interpret it as an integer, the result
                    is usually nonsense.
                </p>
                <p>
                    Actually, the same thing can happen if you read a location in memory
                    incorrectly.  One way that can happen is if you read past the end of
                    an array.
                </p>
                <p>
                    To see what happens, I'll start with a function that allocates an
                    array on the stack and fills it with the numbers from 0 to 99.
                </p>
                <program language="cpp"><code>
                    void f1() {
                        int i;
                        int array[100];

                        for (i=0; i &amp; 100; i++) {
                            array[i] = i;
                        }
                    }
                </code></program>
                <p>
                    Next I'll define a function that creates a smaller array and
                    deliberately accesses elements before the beginning and after
                    the end:
                </p>
                <program language="cpp"><code>
                    void f2() {
                        int x = 17;
                        int array[10];
                        int y = 123;

                        printf("%d\n", array[-2]);
                        printf("%d\n", array[-1]);
                        printf("%d\n", array[10]);
                        printf("%d\n", array[11]);
                    }
                </code></program>
                <p>
                    If I call <c>f1</c> and then <c>f2</c>, I get these results:
                </p>
                <console><output>
                    17
                    123
                    98
                    99
                </output></console>
                <p>
                    The details here depend on the compiler, which arranges variables
                    on the stack.  From these results, we can infer that the
                    compiler put <c>x</c> and <c>y</c> next to each other, <q>below</q>
                    the array (at a lower address).  And when we read past the
                    array, it looks like we are getting values that were left on
                    the stack by the previous function call.
                </p>
                <p>
                    In this example, all of the variables are integers, so it is
                    relatively easy to figure out what is going on.  But in general
                    when you read beyond the bounds of an array, the values you
                    read might have any type.  For example, if I change <c>f1</c>
                    to make an array of floats, the results are:
                </p>
                <console><output>
                    17
                    123
                    1120141312
                    1120272384
                </output></console>
                <p>
                    The latter two values are what you get if you interpret a
                    floating-point value as an integer.  If you encountered this output
                    while debugging, you would have a hard time figuring out what's
                    going on.
                </p>
            </section>

            <section>
                <title>Representing strings</title>

                <p>
                    Related issues sometimes come up with strings.  First, remember
                    that C strings are null-terminated.  When you allocate space
                    for a string, don't forget the extra byte at the end.
                </p>
                <p>
                    Also, the letters <em>and numbers</em> in C strings are
                    encoded in ASCII.  The ASCII codes for the digits <sq>0</sq> through <sq>9</sq>
                    are 48 through 57, <em>not</em> 0 through 9.  The ASCII code 0 is the NUL
                    character that marks the end of a string.  And the ASCII codes 1
                    through 9 are special characters used in some communication protocols.
                    ASCII code 7 is a bell; on some terminals, printing it makes a sound.
                </p>
                <p>
                    The ASCII code for the letter <sq>A</sq> is 65; the code for
                    <sq>a</sq> is 97.  Here are those codes in binary:
                </p>
                <pre>
                    65 = b0100 0001
                    97 = b0110 0001
                </pre>
                <p>
                    A careful observer will notice that they differ by a single
                    bit.  And this pattern holds for the rest of the letters; the
                    sixth bit (counting from the right) acts as a <q>case bit</q>, 0 for
                    upper-case letters and 1 for lower case letters.
                </p>
                <p>
                    As an exercise, write a function that takes a string and converts
                    from lower-case to upper-case by flipping the sixth bit.  As a challenge,
                    you can make a faster version by reading the string 32 or 64 bits
                    at a time, rather than one character at a time.  This optimization
                    is made easier if the length of the string is a multiple of 4 or
                    8 bytes.
                </p>
                <p>
                    If you read past the end of a string, you are likely to see
                    strange characters.  Conversely, if you write a string and
                    then accidentally read it as an <c>int</c> or <c>float</c>, the results
                    will be hard to interpret.
                </p>
                <p>
                    For example, if you run:
                </p>
                <program language="cpp"><code>
                    char array[] = "allen";
                    float *p = array;
                    printf("%f\n", *p);
                </code></program>
                <p>
                    You will find that the ASCII representation of the first 8 characters
                    of my name, interpreted as a double-precision floating point number,
                    is 69779713878800585457664.
                </p>

                <!-- TODO: assert here? -->
            </section>
        </chapter>

        <chapter xml:id="memorymanagement">
            <title>Memory management</title>

            <introduction>
                <p>
                    C provides 4 functions for dynamic memory allocation:
                    <dl>
                        <li>
                            <title><c>malloc</c></title>
                            <p>
                                which takes an integer size, in bytes, and returns
                                a pointer to a newly-allocated chunk of memory with (at least) the
                                given size.  If it can't satisfy the request, it returns
                                the special pointer value <c>NULL</c>.
                            </p>
                        </li>
                        <li>
                            <title><c>calloc</c></title>
                            <p>
                                which is the same as <c>malloc</c> except that
                                it also clears the newly allocated chunk; that
                                is, it sets all bytes in the chunk to 0.
                            </p>
                        </li>
                        <li>
                            <title><c>free</c></title>
                            <p>
                                which takes a pointer to a previously allocated
                                chunk and deallocates it; that is, it makes the space available for
                                future allocation.
                            </p>
                        </li>
                        <li>
                            <title><c>realloc</c></title>
                            <p>
                                which takes a pointer to a previously allocated
                                chunk and a new size.  It allocates a chunk of memory with the new
                                size, copies data from the old chunk to the new, frees the old chunk,
                                and returns a pointer to the new chunk.
                            </p>
                        </li>
                    </dl>
                </p>
                <p>
                    This API is notoriously error-prone and unforgiving.  Memory management
                    is one of the most challenging parts of designing large software systems,
                    which is why most modern languages provide higher-level memory
                    management features like garbage collection.
                </p>
            </introduction>

            <section>
                <title>Memory errors</title>

                <p>
                    The C memory management API is a bit like Jasper Beardly, a minor
                    character on the animated television program <em>The Simpsons</em>;
                    in a few episodes, he appears as a strict substitute teacher who imposes
                    corporal punishment <mdash/> a <q>paddlin'</q> <mdash/> for all infractions.
                </p>
                <p>
                    Here are some of things a program can do that deserve a paddling:
                    <ul>
                        <li>
                            If you access (read or write) any chunk that has not been
                            allocated, that's a paddling.
                        </li>
                        <li>
                            If you free an allocated chunk and then access it, that's
                            a paddling.
                        </li>
                        <li>
                            If you try to free a chunk that has not been allocated,
                            that's a paddling.
                        </li>
                        <li>
                            If you free the same chunk more than once, that's a paddling.
                        </li>
                        <li>
                            If you call <c>realloc</c> with a chunk that was not allocated,
                            or was allocated and then freed, that's a paddling.
                        </li>
                    </ul>
                </p>
                <p>
                    It might not sound difficult to follow these rules, but in a large
                    program a chunk of memory might be allocated in one part of the
                    program, used in several other parts, and freed in yet another
                    part.  So changes in one part of the program can require changes
                    in many other parts.
                </p>
                <p>
                    Also, there might be many aliases, or references to the same allocated
                    chunk, in different parts of the program.  The chunk should not be
                    freed until all references to the chunk are no longer in use.  
                    Getting this right often requires careful analysis across all parts
                    of the program, which is difficult and contrary to fundamental
                    principles of good software engineering.
                </p>
                <p>
                    Ideally, every function that allocates memory should include, as part
                    of the documented interface, information about how that memory is supposed
                    to be freed.  Mature libraries often do this well, but in the real world,
                    software engineering practice often falls short of this ideal.
                </p>
                <p>
                    To make matters worse, memory errors can be difficult
                    to find because the symptoms are unpredictable.  For example:
                    <ul>
                        <li>
                            If you read a value from an unallocated chunk, the system
                            <em>might</em> detect the error, trigger a runtime error called
                            a <term>segmentation fault</term>, and stop the program.
                            Or, the program might read unallocated memory without detecting
                            the error; in that case, the value it gets is whatever happened
                            to be stored at the accessed location, which is unpredictable, and
                            might be different each time the program runs.
                        </li>
                        <li>
                            If you write a value to an unallocated chunk, and don't
                            get a segmentation fault, things are even worse.
                            After you write a value to an invalid location, a long time might
                            pass before it is read and causes problems.  At that point it will
                            be very difficult to find the source of the problem.
                        </li>
                    </ul>
                </p>
                <p>
                    And things can be even worse than that!  One of the most common
                    problems with C-style memory management is that the data structures
                    used to implement <c>malloc</c> and <c>free</c> (which we will see soon)
                    are often stored along with the allocated chunks.  So if you
                    accidentally write past the end of a dynamically-allocated chunk, you
                    are likely to mangle these data structures.  The system usually won't
                    detect the problem until later, when you call <c>malloc</c> or
                    <c>free</c>, and those functions fail in some inscrutable way.
                </p>
                <p>
                    One conclusion you should draw from this is that safe memory
                    management requires design and discipline.  If you write a library
                    or module that allocates memory, you should also provide an
                    interface to free it, and memory management should be part of
                    the API design from the beginning.
                </p>
                <p>
                    If you use a library that allocates memory, you should be disciplined
                    in your use of the API.  For example, if the library provides
                    functions to allocate and deallocate storage, you should use those
                    functions and not, for example, call <c>free</c> on a chunk you did not
                    <c>malloc</c>.  And you should avoid keeping multiple references to the
                    same chunk in different parts of your program.
                </p>
                <p>
                    Often there is a trade-off between safe memory management and performance.
                    For example, the most common source of memory errors is writing 
                    beyond the bounds of an array.  The obvious remedy for this problem
                    is bounds checking; that is, every access to the array should check
                    whether the index is out of bounds.  High-level libraries that provide
                    array-like structures usually perform bounds checking.  But C arrays
                    and most low-level libraries do not.
                </p>
            </section>
            <section xml:id="leak">
                <title>Memory leaks</title>

                <p>
                    There is one more memory error that may or may not deserve a paddling.
                    If you allocate a chunk of memory and never free it, that's a
                    <term>memory leak</term>.
                    <idx>memory leak</idx>
                </p>
                <p>
                    For some programs, memory leaks are ok.  For example, if your program
                    allocates memory, performs computations on it, and then exits, it is
                    probably not necessary to free the allocated memory.  When the program
                    exits, all of its memory is deallocated by the operating system.
                    Freeing memory immediately before exiting might feel more responsible,
                    but it is mostly a waste of time.
                </p>
                <p>
                    But if a program runs for a long time and leaks memory, its total
                    memory use will increase indefinitely.  At that point, a few things
                    might happen:
                    <ul>
                        <li>
                            At some point, the system runs out of physical memory.  On
                            systems without virtual memory, the next call to <c>malloc</c> will
                            fail, returning <c>NULL</c>.
                        </li>
                        <li>
                            On systems with virtual memory, the operating system can move
                            another process's pages from memory to disk and then allocate
                            more space to the leaking process.  I explain this mechanism
                            in <xref ref="paging"/>.
                        </li>
                        <li>
                            There might be a limit on the amount of space a single
                            process can allocate; beyond that, <c>malloc</c> returns <c>NULL</c>.
                        </li>
                        <li>
                            Eventually, a process might fill its virtual address space (or
                            the usable part).  After that, there are no more addresses to
                            allocate, so <c>malloc</c> returns <c>NULL</c>.
                        </li>
                    </ul>
                </p>
                <p>
                    If <c>malloc</c> returns <c>NULL</c>, but you persist and access
                    the chunk you think you allocated, you get a segmentation fault.
                    For this reason, it is considered good style to check the result from
                    <c>malloc</c> before using it.  One option is to add a condition like
                    this after every <c>malloc</c> call:
                </p>
                <program language="cpp"><code>
                    void *p = malloc(size);
                    if (p == NULL) {
                        perror("malloc failed");
                        exit(-1);
                    }
                </code></program>
                <p>
                    <c>perror</c> is declared in <c>stdio.h</c>; it prints
                    an error message and additional information about the last error
                    that occurred.
                </p>
                <p>
                    <c>exit</c>, which is declared in <c>stdlib.h</c>, causes the process
                    to terminate.  The argument is a status code that indicates how
                    the process terminated.  By convention, status code 0 indicates normal
                    termination and -1 indicates an error condition.  Sometimes other
                    codes are used to indicate different error conditions.
                </p>
                <p>
                    Error-checking code can be a nuisance, and it makes programs
                    harder to read.  You can mitigate these problems by wrapping library
                    function calls and their error-checking code in your own
                    functions.  For example, here is a <c>malloc</c> wrapper that checks
                    the return value.
                </p>
                <program language="cpp"><code>
                    void *check_malloc(int size) {
                        void *p = malloc (size);
                        if (p == NULL) {
                            perror("malloc failed");
                            exit(-1);
                        }
                        return p;
                    }
                </code></program>
                <p>
                    Because memory management is so difficult, most large programs, like
                    web browsers, leak memory.  To see which programs on your system are
                    using the most memory, you can use the UNIX utilities <c>ps</c> and
                    <c>top</c>.
                </p>

                <!-- TODO: using Valgrind here? -->
            </section>

            <section>
                <title>Implementation</title>

                <p>
                    When a process starts, the system allocates space for the text segment
                    and statically allocated data, space for the stack, and space for the
                    heap, which contains dynamically allocated data.
                </p>
                <p>
                    Not all programs allocate data dynamically, so the initial size of the
                    heap might be small or zero.  Initially the heap contains only one
                    free chunk.
                </p>
                <p>
                    When <c>malloc</c> is called, it checks whether it can find a free
                    chunk that's big enough.  If not, it has to request more memory
                    from the system.  The function that does that is <c>sbrk</c>,
                    which sets the <term>program break</term>, which you can think
                    of as a pointer to the end of the heap.
                </p>
                <p>
                    When <c>sbrk</c> is called, the OS allocates new pages of physical
                    memory, updates the process's page table, and sets the program break.
                </p>
                <p>
                    In theory, a program could call <c>sbrk</c> directly (without using
                    <c>malloc</c>) and manage the heap itself.  But <c>malloc</c> is easier
                    to use and, for most memory-use patterns, it runs fast and uses memory
                    efficiently.
                </p>
                <p>
                    To implement the memory management API (that is, the functions
                    <c>malloc</c>, <c>free</c>, <c>calloc</c>, and <c>realloc</c>),
                    most Linux systems use <c>ptmalloc</c>,
                    which is based on <c>dlmalloc</c>, written by Doug Lea.  A short paper
                    that describes key elements of the implementation is
                    available
                    <url href="https://gee.cs.oswego.edu/dl/html/malloc.html">from the
                    author</url>.
                </p>
                <p>
                    For programmers, the most important elements to be aware of are:
                    <ul>
                        <li>
                            The run time of <c>malloc</c> does not usually depend on the
                            size of the chunk, but might depend on how many free chunks there
                            are.  <c>free</c> is usually fast, regardless of the number of
                            free chunks.  Because <c>calloc</c> clears every byte in the chunk,
                            the run time depends on chunk size (as well as the number of free
                            chunks).
                        </li>
                        <li>
                            <c>realloc</c> is sometimes fast, if the new size is smaller than the
                            current size, or if space is available to expand the existing chunk.
                            If not, it has to copy data from the old chunk to the new; in that
                            case, the run time depends on the size of the old chunk.
                        </li>
                        <li>
                            <term>Boundary tags</term>: When <c>malloc</c> allocates a chunk, it adds
                            space at the beginning and end to store information about the chunk,
                            including its size and the state (allocated or free).  These bits of
                            data are called <term>boundary tags</term>.  Using these tags,
                            <c>malloc</c> can get from any chunk to the previous chunk and the
                            next chunk in  memory.  In addition, free chunks are chained into
                            a doubly-linked   list; each free chunk contains pointers to the next
                            and previous chunks in the <term>free list</term>.
                        </li>
                        <li>
                            The boundary tags and free list pointers make up <c>malloc</c>'s
                            internal data structures.  These data structures are interspersed with
                            program data, so it is easy for a program error to damage them.
                        </li>
                        <li>
                            <term>Space overhead</term>: Boundary tags and free list pointers take up
                            space.  The minimum chunk size on most systems is 16 bytes.  So for
                            very small chunks, <c>malloc</c> is not space efficient.  If your
                            program requires large numbers of small structures, it might be more
                            efficient to allocate them in arrays.
                        </li>
                        <li>
                            <term>Fragmentation</term>: If you allocate and free chunks with varied
                            sizes, the heap will tend to become fragmented.  That is, the free
                            space might be broken into many small pieces.  Fragmentation wastes
                            space; it also slows the program down by making memory caches less
                            effective.
                        </li>
                        <li>
                            <term>Binning and caching</term>: The free list is sorted by size into bins,
                            so when <c>malloc</c> searches for a chunk with a particular size, it
                            knows what bin to search in.  If you free a chunk and then
                            immediately allocate a chunk with the same size, <c>malloc</c> will
                            usually be fast.
                        </li>
                    </ul>
                </p>
            </section>                
        </chapter>

        <chapter xml:id="caching">
            <title>Caching</title>

            <!-- TODO: move this model of computer hardware to Chapter 1 -->
            <!-- TODO: talk about CPU modes, either here or in Chapter 1 -->

            <section>
                <title>How programs run</title>

                <p>
                    In order to understand caching, you have to understand how computers
                    execute programs.  For a deep understanding of this topic, you should
                    study computer architecture.  My goal in this chapter is to provide
                    a simple model of program execution.
                </p>
                <p>
                    When a program starts, the code (or text) is usually on a hard disk
                    or solid state drive.  The operating system creates a new process to
                    run the program, then the <term>loader</term>
                    copies the text from storage into main memory and starts the program by
                    calling <c>main</c>.
                </p>
                <p>
                    While the program is running, most of its data is stored in main
                    memory, but some of the data is in registers, which are
                    small units of memory on the CPU.  These registers include:
                    <ul>
                        <li>
                            The program counter, or PC, which contains the address (in
                            memory) of the next instruction in the program.
                        </li>
                        <li>
                            The instruction register, or IR, which contains the machine code
                            instruction currently executing.
                        </li>
                        <li>
                            The stack pointer, or SP, which contains the address of the
                            stack frame for the current function, which contains its parameters
                            and local variables.
                        </li>
                        <li>
                            General-purpose registers that hold the data the program is
                            currently working with.
                        </li>
                        <li>
                            A status register, or flag register, that contains information
                            about the current computation.  For example, the flag register
                            usually contains a bit that is set if the result of the previous
                            operation was zero.
                        </li>
                    </ul>
                </p>

                <p>
                    When a program is running, the CPU executes the following steps,
                    called the <term>instruction cycle</term><idx>instruction cycle</idx>:

                    <dl width="narrow">
                        <li>
                            <title>Fetch</title>
                            <p>
                                The next instruction is fetched from memory and stored
                                in the instruction register.
                            </p>
                        </li>
                        <li>
                            <title>Decode</title>
                            <p>
                                Part of the CPU, called the <term>control unit</term>, decodes
                                the instruction and sends signals to the other parts of
                                the CPU.
                            </p>
                        </li>
                        <li>
                            <title>Execute</title>
                            <p>
                                Signals from the control unit cause the appropriate
                                computation to occur.
                            </p>
                        </li>
                    </dl>
                </p>
                <p>
                    Most computers can execute a few hundred different instructions,
                    called the <term>instruction set</term>.  But most instructions fall
                    into a few general categories:
                    <idx>instruction set</idx>
                    <dl width="narrow">
                        <li>
                            <title>Load</title>
                            <p>
                                Transfers a value from memory to a register.
                            </p>
                        </li>
                        <li>
                            <title>Arithmetic/logic</title>
                            <p>
                                Loads operands from registers, performs
                                a mathematical operation, and stores the result in a register.
                            </p>
                        </li>
                        <li>
                            <title>Store</title>
                            <p>
                                Transfers a value from a register to memory.
                            </p>
                        </li>
                        <li>
                            <title>Jump/branch</title>
                            <p>
                                Changes the program counter, causing the flow
                                of execution to jump to another location in the program.  Branches
                                are usually conditional, which means that they check a flag
                                in the flag register and jump only if it is set.
                            </p>
                        </li>
                    </dl>
                </p>
                <p>
                    Some instructions sets, including the ubiquitous x86, provide
                    instructions that combine a load and an arithmetic operation.
                </p>
                <p>
                    During each instruction cycle, one instruction is read from the
                    program text.  In addition, about half of the instructions in a
                    typical program load or store data.  And therein
                    lies one of the fundamental problems of computer architecture: the
                    <term>memory bottleneck</term>.
                </p>
                <p>

                    In current computers, a typical core is capable of executing an
                    instruction in less than 1<nbsp/>ns.  But the time it takes to
                    transfer data to and from memory is about 100<nbsp/>ns.  If the
                    CPU has to wait 100<nbsp/>ns to fetch the next instruction, and
                    another 100 ns to load data, it would complete instructions 200
                    times slower than what's theoretically possible.  For many
                    computations, memory is the speed limiting factor, not the CPU.
                </p>
            </section>

            <section>
                <title>Cache performance</title>
                
                <p>
                    The solution to this problem, or at least a partial solution, is
                    caching.  A <term>cache</term> is a small, fast memory that is physically close
                    to the CPU, usually on the same chip.
                    <idx>cache</idx>
                </p>

                <!-- %TODO: Clean up these paragraphs -->

                <p>
                    Actually, current computers typically have several levels of cache:
                    the Level 1 cache, which is the smallest and fastest, might be
                    1<ndash/>2<nbsp/>MiB with a access times near 1<nbsp/>ns;
                    the Level 2 cache might have access times near 4<nbsp/>ns,
                    and the Level 3 cache might take 16<nbsp/>ns.
                </p>
                <p>
                    When the CPU loads a value from memory, it stores a copy in the cache.
                    If the same value is loaded again, the CPU gets the cached copy
                    and doesn't have to wait for memory.
                </p>
                <p>
                    Eventually the cache gets full.  Then, in order to bring something
                    new in, we have to kick something out.  So if the CPU loads a value
                    and then loads it again much later, it might not be in cache any more.
                </p>
                <p>
                    The performance of many programs is limited by the effectiveness
                    of the cache.  If the instructions and data needed by the CPU are usually
                    in cache, the program can run close to the full speed of the CPU.
                    If the CPU frequently needs data that are not in cache, the program is
                    limited by the speed of memory.
                </p>
                <p>
                    The cache <term>hit rate</term>, <m>h</m>, is the fraction of memory accesses that
                    find data in cache; the <term>miss rate</term>, <m>m</m>, is the fraction of memory
                    accesses that have to go to memory.  If the time to process a cache
                    hit is <m>T_h</m> and the time for a cache miss is <m>T_m</m>, the average time
                    for each memory access is
                    <me> h T_h + m T_m</me>
                    <idx>hit rate</idx>
                    <idx>miss rate</idx>
                </p>
                <p>
                    Equivalently, we could define the <term>miss penalty</term> as the extra
                    time to process a cache miss, <m>T_p = T_m - T_h</m>.  Then the average access
                    time is
                    <me>T_h + m T_p</me>
                    <idx>miss penalty</idx>
                </p>
                <p>
                    When the miss rate is low, the average access time can be close to
                    <m>T_h</m>.  That is, the program can perform as if memory ran at
                    cache speeds.
                </p>
            </section>

            <section>
                <title>Locality</title>

                <p>
                    When a program reads a byte for the first time, the cache usually
                    loads a <q>block</q> or <q>line</q> of data that includes the requested
                    byte and some of its neighbors.  If the program goes on to read one
                    of the neighbors, it will already be in cache.
                </p>
                <p>
                    As an example, suppose the block size is 64 B;
                    you read a string with length 64, and the first
                    byte of the string happens to fall at the beginning of a block.  When
                    you load the first byte, you incur a miss penalty, but
                    after that the rest of the string will be in cache.  After
                    reading the whole string, the hit rate will be <m>63/64</m>, about 98%.
                    If the string spans two blocks, you would incur 2 miss penalties.  But
                    even then the hit rate would be <m>62/64</m>, or almost 97%.  If you then
                    read the same string again, the hit rate would be 100%.
                </p>
                <p>
                    On the other hand, if the program jumps around unpredictably,
                    reading data from scattered locations in memory, and seldom
                    accessing the same location twice, cache performance would be
                    poor.
                </p>
                <p>
                    The tendency of a program to use the same data more than once is
                    called <term>temporal locality</term>.  The tendency to use data in nearby
                    locations is called <term>spatial locality</term>.  Fortunately, many
                    programs naturally display both kinds of locality:

                    <ul>
                        <li>
                            Most programs contain blocks of code with no jumps or
                            branches.  Within these blocks, instructions run
                            sequentially, so the access pattern has
                            spatial locality.
                        </li>
                        <li>
                            In a loop, programs execute the same instructions many
                            times, so the access pattern has temporal locality.
                        </li>
                        <li>
                            The result of one instruction is often used immediately as
                            an operand of the next instruction, so the data access pattern
                            has temporal locality.
                        </li>
                        <li>
                            When a program executes a function, its parameters and local
                            variables are stored together on the stack; accessing these values
                            has spatial locality.
                        </li>
                        <li>
                            One of the most common processing patterns is to read or write
                            the elements of an array sequentially; this pattern also has
                            spatial locality.
                        </li>
                    </ul>
                </p>
                <p>
                    The next section explores the relationship
                    between a program's access pattern and cache performance.
                </p>
            </section>

            <section>
                <title>Measuring cache performance</title>

                <p>
                    When I was a graduate student at U.C. Berkeley I was a teaching
                    assistant for Computer Architecture with Brian Harvey.  One of my
                    favorite exercises involved a program that iterates through an array
                    and measures the average time to read and write an element.  By
                    varying the size of the array, it is possible to infer the size
                    of the cache, the block size, and some other attributes.
                </p>
                <p>
                    My modified version of this program is in the <c>cache</c> directory
                    of the repository for this book (see <xref ref="code"/>).
                </p>
                <p>
                    The important part of the program is this loop:
                </p>
                <program language="cpp"><code><![CDATA[
                    iters = 0;
                    do {
                        sec0 = get_seconds();

                        for (index = 0; index < limit; index += stride) 
                            array[index] = array[index] + 1;
        
                        iters = iters + 1; 
                        sec = sec + (get_seconds() - sec0);
        
                    } while (sec < 0.1);
                ]]></code></program>
                <p>
                    The inner <c>for</c> loop traverses the array.  <c>limit</c>
                    determines how much of the array it traverses; <c>stride</c>
                    determines how many elements it skips over.  For example, if
                    <c>limit</c> is 16 and <c>stride</c> is 4, the loop would access
                    elements 0, 4, 8, and 12.
                </p>
                <p>
                    <c>sec</c> keeps track of the total CPU time used by the inner loop.
                    The outer loop runs until <c>sec</c> exceeds 0.1 seconds, which is
                    long enough that we can compute the average time with sufficient
                    precision.
                </p>
                <p>
                    <c>get_seconds</c> uses the system call <c>clock_gettime</c>,
                    converts to seconds, and returns the result as a <c>double</c>:
                </p>
                <program language="cpp"><code><![CDATA[
                    double get_seconds(){
                        struct timespec ts;
                        clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ts);
                        return ts.tv_sec + ts.tv_nsec / 1e9;
                    }
                ]]></code></program>
                <p>
                    To isolate the time to access the elements of the array,
                    the program runs a second loop that is almost identical except
                    that the inner loop doesn't touch the array; it always increments
                    the same variable:
                </p>
                <program language="cpp"><code><![CDATA[
                    iters2 = 0;
                    do {
                        sec0 = get_seconds();
        
                        for (index = 0; index < limit; index += stride) 
                            temp = temp + index;
        
                        iters2 = iters2 + 1;
                        sec = sec - (get_seconds() - sec0);

                    } while (iters2 < iters);
                ]]></code></program>    
                <p>
                    The second loop runs the same number of iterations as the first.
                    After each iteration, it <em>subtracts</em> the elapsed time from
                    <c>sec</c>.  When the loop completes, <c>sec</c> contains the total
                    time for all array accesses, minus the total time it took to increment
                    <c>temp</c>.  This difference is the total miss penalty incurred by
                    all accesses.  Finally, we divide by the number of accesses to
                    get the average miss penalty per access, in ns:
                </p>
                <pre>
                    sec * 1e9 / iters / limit * stride
                </pre>
                <p>
                    If you compile and run <c>cache.c</c> you should see output like this:
                </p>
                <console><output>
Size:    4096 Stride:       8 read+write: 0.8633 ns
Size:    4096 Stride:      16 read+write: 0.7023 ns
Size:    4096 Stride:      32 read+write: 0.7105 ns
Size:    4096 Stride:      64 read+write: 0.7058 ns
                </output></console>
                <p>
                    If you have Python and <c>matplotlib</c> installed, you can use
                    <c>graph_data.py</c> to graph the results.  <xref ref="cachedata"/>
                    shows the results when I ran it on a Dell Optiplex 7010.
                    Notice that the array size and stride are reported in
                    bytes, not number of array elements.
                </p>
                <figure xml:id="cachedata">
                    <caption>Average miss penalty as a function of array size and stride.</caption>
                    <image source="cache_data.svg"/>
                </figure>
                <p>
                    Take a minute to consider this graph, and see what you can infer
                    about the cache.  Here are some things to think about:

                    <ul>
                        <li>
                            The program reads through the array many times, so it has plenty
                            of temporal locality.  If the entire array fits in cache, we expect
                            the average miss penalty to be near 0.
                        </li>
                        <li>
                            When the stride is 4 bytes, we read every element of the array,
                            so the program has plenty of spatial locality.  If the block size is
                            big enough to contain 64 elements, for example, the hit rate would
                            be 63/64, even if the array does not fit in cache.
                        </li>
                        <li>
                            If the stride is equal to the block size (or greater), the
                            spatial locality is effectively zero, because each time we read a
                            block, we only access one element.  In that case we expect to see
                            the maximum miss penalty.
                        </li>
                    </ul>
                </p>
                <p>
                    In summary, we expect good cache performance if the array is smaller
                    than the cache size <em>or</em> if the stride is smaller than the block
                    size.  Performance only degrades if the array is bigger than the
                    cache <em>and</em> the stride is large.
                </p>
                <p>
                    In <xref ref="cachedata"/>, cache performance is good, for all strides,
                    as long as the array is less than <m>2^{22}</m><nbsp/>B.  We can infer that the
                    cache size is near 4 MiB; in fact, according to the specs,
                    it is 3<nbsp/>MiB.
                </p>
                <p>
                    When the stride is 8, 16, or 32 B, cache performance is good.  At 64 B
                    it starts to degrade, and for larger strides the average miss
                    penalty is about 9 ns.  We can infer that the block size near 128 B.
                </p>
                <p>
                    Many processors use <q>multi-level caches</q> that include a small,
                    fast cache and a bigger, slower cache.  In this example, it looks 
                    like the miss penalty increases a little when the array size is bigger
                    than <m>2^{14}</m><nbsp/>B, so it's possible that this processor also
                    has a 16 KB cache with an access time less than 1 ns.
                </p>
            </section>

            <section>
                <title>Programming for cache performance</title>

                <p>
                    Memory caching is implemented in hardware, so most of the time
                    programmers don't need to know much about it.  But if you know how
                    caches work, you can write programs that use them more effectively.
                </p>
                <p>
                    For example, if you are working with a large array, it might be
                    faster to traverse the array once, performing several operations with
                    each element, rather than traversing the array several times.
                </p>
                <p>
                    If you are working with a 2-D array, it might be stored as an array
                    of rows.  If you traverse through the elements, it would be faster
                    to go row-wise, with stride equal to the element size, rather
                    than column-wise, with stride equal to the row length.
                </p>
                <p>
                    Linked data structures don't always exhibit spatial locality, because
                    the nodes aren't necessarily contiguous in memory.  But if you allocate
                    many nodes at the same time, they are usually co-located in the heap.
                    Or, even better, if you allocate an array of nodes all at once, you
                    know they will be contiguous.
                </p>
                <p>
                    Recursive strategies like mergesort often have good cache behavior
                    because they break big arrays into smaller pieces and then work
                    with the pieces.  Sometimes these algorithms can be tuned to take
                    advantage of cache behavior.
                </p>
                <p>
                    For applications where performance is critical, it is possible
                    to design algorithms tailored to the size of the cache, the block size,
                    and other hardware characterstics.  Algorithms like that are
                    called <q>cache-aware</q>.  The obvious drawback of cache-aware
                    algorithms is that they are hardware-specific.
                </p>
            </section>

            <section>
                <title>The memory hierarchy</title>

                <p>
                    At some point during this chapter, a question like the following
                    might have occurred to you:
                </p>
                <blockquote>
                    <p>
                        If caches are so much faster than main memory, why not make a
                        really big cache and forget about memory?
                    </p>
                </blockquote>
                <p>
                    Without going too far into computer architecture, there are two
                    reasons: electronics and economics.  Caches are fast because they are
                    small and close to the CPU, which minimizes delays due to capacitance
                    and signal propagation.  If you make a cache big, it will be slower.
                </p>
                <p>
                    Also, caches take up space on the processor chip, and bigger chips are
                    more expensive.  Main memory is usually dynamic random-access memory
                    (DRAM), which uses only one transistor and one capacitor per bit, so
                    it is possible to pack more memory into the same amount of space.  But
                    this way of implementing memory is slower than the way caches are
                    implemented.
                </p>
                <p>
                    Also main memory is usually packaged in a dual in-line memory module
                    (DIMM) that includes 16 or more chips.  Several small chips are cheaper
                    than one big one.
                </p>
                <p>
                    The trade-off between speed, size, and cost is the fundamental reason
                    for caching.  If there were one memory technology that was fast,
                    big, and cheap, we wouldn't need anything else.
                </p>
                <p>
                    The same principle applies to storage as well as memory.
                    Solid state drives (SSD) are fast, but they are more expensive than
                    hard drives (HDD), so they tend to be smaller.  Tape drives are even
                    slower than hard drives, but they can store large amounts of data
                    relatively cheaply.
                </p>
                <p>
                    The following table shows typical access times, sizes, and 
                    costs for each of these technologies.  
                </p>
                <table xml:id="tab-techcosts">
                    <title>Memory technology cost and size</title>
                    <tabular>
                        <row header="yes" bottom="minor">
                            <cell>Device</cell>
                            <cell>Access time</cell>
                            <cell>Typical size</cell>
                            <cell>Cost</cell>
                        </row>
                        <row>
                            <cell>Register</cell>
                            <cell>0.5 ns</cell>
                            <cell>256 B</cell>
                            <cell>?</cell>
                        </row>
                        <row>
                            <cell>Cache</cell>
                            <cell>1 ns</cell>
                            <cell>2 MiB</cell>
                            <cell>?</cell>
                        </row>
                        <row>
                            <cell>DRAM</cell>
                            <cell>100 ns</cell>
                            <cell>4 GiB</cell>
                            <cell>$10 / GiB</cell>
                        </row>
                        <row>
                            <cell>SSD</cell>
                            <cell>10 &#956;s</cell>
                            <cell>100 GiB</cell>
                            <cell>$1 / GiB</cell>
                        </row>
                        <row>
                            <cell>HDD</cell>
                            <cell>5 ms</cell>
                            <cell>500 GiB</cell>
                            <cell>$0.25 / GiB</cell>
                        </row>
                        <row>
                            <cell>Tape</cell>
                            <cell>minutes</cell>
                            <cell>1<ndash/>2 TiB</cell>
                            <cell>$0.02 / GiB</cell>
                        </row>
                    </tabular>
                </table>
                <p>
                    The number and size of registers depends on details of the
                    architecture.  Current computers have about 32 general-purpose
                    registers, each storing one <q>word</q>.  On a 32-bit computer, a word
                    is 32 bits or 4 B.  On a 64-bit computer, a word is 64 bits or 8 B.
                    So the total size of the register file is 100<ndash/>300 B.
                </p>
                <p>
                    The cost of registers and caches is hard to quantify.  They contribute
                    to the cost of the chips they are on, but consumers don't see that
                    cost directly.
                </p>
                <p>
                    For the other numbers in the table, I looked at the specifications for
                    typical hardware for sale from online computer hardware stores.  By
                    the time you read this, these numbers will be obsolete, but they give
                    you an idea of what the performance and cost gaps looked like at one
                    point in time.
                </p>
                <p>
                    These technologies make up the <term>memory hierarchy</term> (note that this
                    use of <q>memory</q> also includes storage).  Each
                    level of the hierarchy is bigger and slower than the one above it.
                    And in some sense, each level acts as a cache for the one below
                    it.  You can think of main memory as a cache for programs and data
                    that are stored permanently on SSDs and HHDs.  And if you are working
                    with very large datasets stored on tape, you could use hard drives
                    to cache one subset of the data at a time.
                </p>
            </section>

            <section>
                <title>Caching policy</title>

                <p>
                    The memory hierarchy suggests a framework for thinking about
                    caching.  At every level of the hierarchy, we have to address
                    four fundamental questions of caching:
                    <ul>
                        <li>
                            <em>Who moves data up and down the hierarchy?</em>  At the top of the
                            hierarchy, register allocation is usually done by the compiler.
                            Hardware on the CPU handles the memory cache.  Users implicitly move
                            data from storage to memory when they execute programs and open
                            files.  But the operating system also moves data back and forth
                            between memory and storage.  At the bottom of the hierarchy,
                            administrators move data explicitly between disk and tape.
                        </li>
                        <li>
                            <em>What gets moved?</em>  In general, block sizes are small at the top
                            of the hierarchy and bigger at the bottom.  In a memory cache, a
                            typical block size is 128 B.  Pages in memory might be 4 KiB, but
                            when the operating system reads a file from disk, it might read 10s
                            or 100s of blocks at a time.
                        </li>
                        <li>
                            <em>When does data get moved?</em>  In the most basic cache, data gets
                            moved into cache when it is used for the first time.  But many
                            caches use some kind of <term>prefetching</term>, meaning that data is
                            loaded before it is explicitly requested.  We have already seen
                            one form of prefetching: loading an entire block when only part of
                            it is requested.
                            <idx>prefetch</idx>
                        </li>
                        <li>
                            <em>Where in the cache does the data go?</em>  When the cache is full, we
                            can't bring anything in without kicking something out.  Ideally,
                            we want to keep data that will be used again soon and replace data
                            that won't.
                        </li>
                    </ul>
                </p>
                <p>
                    The answers to these questions make up the <term>cache policy</term>.
                    Near the top of the hierarchy, cache policies tend to be simple
                    because they have to be fast and they are implemented in hardware.
                    Near the bottom of the hierarchy, there is more time to make decisions,
                    and well-designed policies can make a big difference.
                    <idx>cache policy</idx>
                </p>
                <p>
                    Most cache policies are based on the principle that history repeats
                    itself; if we have information about the recent past, we can use it to
                    predict the immediate future.  For example, if a block of data has
                    been used recently, we expect it to be used again soon.  This
                    principle suggests a replacement policy called
                    <term>least recently used</term>, or LRU, which removes from the cache
                    a block of data that has not been used recently.  For more on this topic,
                    see <url href="https://en.wikipedia.org/wiki/Cache_algorithms">Cache algorithms</url>.
                    <idx>least recently used</idx>
                    <idx>LRU</idx>
                </p>
            </section>

            <section xml:id="paging">
                <title>Paging</title>

                <p>
                    In systems with virtual memory, the operating system can move
                    pages back and forth between memory and storage.  As I mentioned
                    in <xref ref="leak"/>, this mechanism is called <term>paging</term> or
                    sometimes <term>swapping</term>.
                    <idx>paging</idx>
                    <idx>swapping</idx>
                </p>
                <p>
                    Here's how the process works:
                    <ol>
                        <li>
                            Suppose Process A calls <c>malloc</c> to allocate a chunk.  If there
                            is no free space in the heap with the requested size, <c>malloc</c> calls
                            <c>sbrk</c> to ask the operating system for more memory.
                        </li>
                        <li>
                            If there is a free page in physical memory, the operating system
                            adds it to the page table for Process A, creating a new range of valid
                            virtual addresses.
                        </li>
                        <li>
                            If there are no free pages, the paging system chooses a
                            <term>victim page</term> belonging to Process B.  It copies the
                            contents of the victim page from memory to disk, then it modifies
                            the page table for Process B to indicate that this page is <q>swapped out</q>.
                            <idx>victim page</idx>
                        </li>
                        <li>
                            Once the data from Process B is written, the page can be reallocated
                            to Process A.  To prevent Process A from reading Process B's data, the
                            page should be cleared (filled with zeros).
                        </li>
                        <li>
                            At this point the call to <c>sbrk</c> can return, giving <c>malloc</c>
                            additional space in the heap.  Then <c>malloc</c> allocates the requested
                            chunk and returns.  Process A can resume.
                        </li>
                        <li>
                            When Process A completes, or is interrupted, the scheduler might
                            allow Process B to resume.  When Process B accesses a page that has been
                            swapped out, the memory management unit notices that the page is
                            <q>invalid</q> and causes an interrupt.
                        </li>
                        <li>
                            When the operating system handles the interrupt, it sees that
                            the page is swapped out, so it transfers the page back from disk to
                            memory.  
                        </li>
                        <li>
                            Once the page is swapped in, Process B can resume.
                        </li>
                    </ol>
                </p>
                <p>
                    When paging works well, it can greatly improve the utilization of
                    physical memory, allowing more processes to run in less space.
                    Here's why:
                    <ul>
                        <li>
                            Most processes don't use all of their allocated memory.  Many
                            parts of the text segment are never executed, or execute once and
                            never again.  Those pages can be swapped out without causing any
                            problems.
                        </li>
                        <li>
                            If a program leaks memory, it might leave allocated space behind
                            and never access it again.  By swapping those pages out, the
                            operating system can effectively plug the leak.
                        </li>
                        <li>
                            On most systems, there are processes like daemons that sit idle
                            most of the time and only occasionally <q>wake up</q> to respond to
                            events.  While they are idle, these processes can be swapped out.
                        </li>
                        <li>
                            A user might have many windows open, but only a few are active
                            at a time.  The inactive processes can be swapped out.
                        </li>
                        <li>
                            Also, there might be many processes running the same program.
                            These processes can share the same text and static segments, avoiding
                            the need to keep multiple copies in physical memory.
                        </li>
                    </ul>
                </p>
                <p>
                    If you add up the total memory allocated to all processes, it can
                    greatly exceed the size of physical memory, and yet the system can
                    still behave well.
                </p>
                <p>
                    Up to a point.
                </p>
                <p>
                    When a process accesses a page that's swapped out, it has to get the
                    data back from disk, which can take several milliseconds.  The
                    delay is often noticeable.  If you leave a window idle for a long
                    time and then switch back to it, it might start slowly,
                    and you might hear the disk drive working while pages are
                    swapped in.  
                </p>
                <p>
                    Occasional delays like that might be acceptable, but if you have too
                    many processes using too much space, they start to interfere with each
                    other.  When Process A runs, it evicts the pages Process B needs.
                    Then when B runs, it evicts the pages A needs.  When this happens,
                    both processes slow to a crawl and the system can become unresponsive.
                    This scenario is called <term>thrashing</term>.
                </p>
                <p>
                    In theory, operating systems could avoid thrashing by detecting an
                    increase in paging and blocking or killing processes until the system
                    is responsive again.  But as far as I can tell, most systems don't do
                    this, or don't do it well; it is often left to users to limit their
                    use of physical memory or try to recover when thrashing occurs.
                </p>
            </section>
        </chapter>

        <chapter xml:id="multitasking">
            <title>Multitasking</title>
            <introduction>
                <p>
                    In many current systems, the CPU contains multiple cores, which means
                    it can run several processes at the same time.  In addition, each core
                    is capable of <term>multitasking</term>, which means it can switch from one
                    process to another quickly, creating the illusion that many processes
                    are running at the same time.
                    <idx>multitasking</idx>
                </p>
                <p>
                    The part of the operating system that implements multitasking is
                    the <term>kernel</term>.  In a nut or seed, the kernel is the innermost
                    part, surrounded by a shell.  In an operating system, the kernel
                    is the lowest level of software, surrounded by several other
                    layers, including an interface called a <term>shell</term>.
                    Computer scientists love extended metaphors.
                    <idx>kernel</idx>
                    <idx>shell</idx>
                </p>
                <p>
                    At its most basic, the kernel's job is to
                    handle interrupts.  An <term>interrupt</term> is an event that stops the
                    normal instruction cycle and causes the flow of execution to jump to a
                    special section of code called an <term>interrupt handler</term>.
                    <idx>interrupt</idx>
                    <idx>interrupt handler</idx>
                </p>
                <p>
                    A <term>hardware interrupt</term> is caused when a device sends a signal to the
                    CPU.  For example, a network interface might cause an interrupt when
                    a packet of data arrives, or a disk drive might cause an interrupt
                    when a data transfer is complete.  Most systems also have timers that
                    cause interrupts at regular intervals, or after an elapsed time.
                </p>
                <p>
                    A <term>software interrupt</term> is caused by a running program.  For example, if
                    an instruction cannot complete for some reason, it might trigger an
                    interrupt so the condition can be handled by the operating system.
                    Some floating-point errors, like division by zero, are handled
                    using interrupts.
                    <idx>software interrupt</idx>
                </p>
                <p>
                    When a program needs to access a hardware device,
                    it makes a <term>system call</term>, which is similar to a function call,
                    except that instead of jumping to the beginning of the function,
                    it executes a special instruction that triggers an interrupt, causing
                    the flow of execution to jump to the kernel.  The kernel reads the
                    parameters of the system call, performs the requested operation,
                    and then resumes the interrupted process.
                    <idx>system call</idx>
                </p>
            </introduction>
            
            <section>
                <title>Hardware state</title>

                <p>
                    Handling interrupts requires cooperation between hardware and
                    software.  When an interrupt occurs, there might be several
                    instructions running on the CPU, data stored in registers, and
                    other <term>hardware state</term>.
                    <idx>hardware state</idx>
                </p>
                <p>
                    Usually the hardware is responsible for bringing the CPU
                    to a consistent state; for example, every instruction should either
                    complete or behave as if it never started.  No instruction should
                    be left half complete.  Also, the hardware is responsible for
                    saving the program counter (PC), so the kernel knows where to
                    resume.
                </p>
                <p>
                    Then, usually, it is the responsibility of the interrupt handler
                    to save the rest of the hardware state before it does anything that
                    might modify it, and then restore the saved state before the interrupted
                    process resumes.
                </p>
                <p>
                    Here is an outline of this sequence of events:
                    <ol>
                        <li>
                            When the interrupt occurs, the hardware saves the program
                            counter in a special register and jumps to the appropriate interrupt
                            handler.
                        </li>
                        <li>
                            The interrupt handler stores the program counter and the
                            status register in memory, along with the contents of any data
                            registers it plans to use.
                        </li>
                        <li>
                            The interrupt handler runs whatever code is needed to handle
                            the interrupt.
                        </li>
                        <li>
                            Then it restores the contents of the saved registers.  Finally,
                            it restores the program counter of the interrupted process, which
                            has the effect of jumping back to the interrupted instruction.
                        </li>
                    </ol>
                </p>
                <p>
                    If this mechanism works correctly, there is generally no way for
                    the interrupted process to know there was an interrupt, unless
                    it detects the change in time between instructions.
                </p>
            </section>

            <section>
                <title>Context switching</title>

                <p>
                    Interrupt handlers can be fast because they don't have to save the
                    entire hardware state; they only have to save registers they are
                    planning to use.
                </p>
                <p>
                    But when an interrupt occurs, the kernel does not always resume the
                    interrupted process.  It has the option of switching to another
                    process.  This mechanism is called a <term>context switch</term>.
                    <idx>context switch</idx>
                </p>
                <p>
                    In general, the kernel doesn't know which registers a process will
                    use, so it has to save all of them.  Also, when it switches to a new
                    process, it might have to clear data stored in the memory management
                    unit (see <xref ref="address_translation"/>).
                    And after the context switch, it
                    might take some time for the new process to load data into the cache.
                    For these reasons, context switches are relatively slow, on the order
                    of thousands of cycles, or a few microseconds.
                </p>
                <p>
                    In a multi-tasking system, each process is allowed to run for a short
                    period of time called a <term>time slice</term> or <term>quantum</term>.
                    During a context switch, the kernel sets a hardware timer that causes
                    an interrupt at the end of the time slice.  When the interrupt
                    occurs, the kernel can switch to another process or allow the
                    interrupted process to resume.  The part of the operating system
                    that makes this decision is the <term>scheduler</term>.
                    <idx>time slice</idx>
                    <idx>quantum</idx>
                    <idx>scheduler</idx>
                </p>
            </section>

            <section>
                <title>The process life cycle</title>

                <p>
                    When a process is created, the operating system allocates a
                    data structure that contains information about the process, called
                    a <q>process control block</q> or PCB.  Among other things, the
                    PCB keeps track of the process state, which is one of:
                    <dl width="narrow">
                        <li>
                            <title>Running</title>
                            <p>
                                if the process is currently running on a core.
                            </p>
                        </li>
                        <li>
                            <title>Ready</title>
                            <p>
                                if the process could be running, but isn't, usually because
                                there are more runnable processes than cores.
                            </p>
                        </li>
                        <li>
                            <title>Blocked</title>
                            <p>
                                if the process cannot run because it is waiting for
                                a future event like network communication or a disk read.
                            </p>
                        </li>
                        <li>
                            <title>Done</title>
                            <p>
                                if the process has completed, but has exit status
                                information that has not been read yet.
                            </p>
                        </li>
                    </dl>
                </p>
                <p>
                    Here are the events that cause a process to transition from one state to another:
                    <ul>
                        <li>
                            A process is created when the running program executes a system
                            call like <c>fork</c>.  At the end of the system call, the new
                            process is usually ready.  Then the scheduler might resume the
                            original process (the <term>parent</term>) or start the new process (the
                            <term>child</term>).
                        </li>
                        <li>
                            When a process is started or resumed by the scheduler, its state
                            changes from ready to running.
                        </li>
                        <li>
                            When a process is interrupted and the scheduler chooses not
                            to let it resume, its state changes from running to ready.
                        </li>
                        <li>
                            If a process executes a system call that cannot complete
                            immediately, like a disk request, it becomes blocked
                            and the scheduler usually chooses another process.
                        </li>
                        <li>
                            When an operation like a disk request completes, it causes an
                            interrupt.  The interrupt handler figures out which process was
                            waiting for the request and switches its state from
                            blocked to ready.  Then the scheduler may or may not choose to
                            resume the unblocked process.
                        </li>
                        <li>
                            When a process calls <c>exit</c>, the interrupt handler stores
                            the exit code in the PCB and changes the process's state to done.
                        </li>
                    </ul>
                </p>
            </section>

            <section>
                <title>Scheduling</title>

                <p>
                    As we saw in <xref ref="unixps"/> there might be hundreds of
                    processes on a computer, but usually most of them are blocked.  Most
                    of the time, there are only a few processes that are ready or running.
                    When an interrupt occurs, the scheduler decides which process to start
                    or resume.
                </p>
                <p>
                    On a workstation or laptop, the primary goal of the scheduler is to
                    minimize response time; that is, the computer should respond quickly
                    to user actions.  Response time is also important on a server, but in
                    addition the scheduler might try to maximize throughput, which is the
                    number of requests that complete per unit of time.
                </p>
                <p>
                    Usually the scheduler doesn't have much information about what
                    processes are doing, so its decisions are based on a few
                    heuristics:
                    <ul>
                        <li>
                            Processes might be limited by different resources.  A process
                            that does a lot of computation is probably CPU-bound, which means that
                            its run time depends on how much CPU time it gets.  A process that
                            reads data from a network or disk might be I/O-bound, which means that
                            it would run faster if data input and output went faster, but would not
                            run faster with more CPU time.  Finally, a process that interacts with
                            the user is probably blocked, most of the time, waiting for user actions.
                        </li>
                        <li> <!-- TODO: this wasn't an item before -->
                            The operating system can sometimes classify processes based on their
                            past behavior, and schedule them accordingly.  For example, when an
                            interactive process is unblocked, it should probably run immediately,
                            because a user is probably waiting for a reply.  On the other hand,
                            a CPU-bound process that has been running for a long time might be
                            less time-sensitive.
                        </li>
                        <li>
                            If a process is likely to run for a short time and then make
                            a blocking request, it should probably run immediately, for two reasons:
                            (1) if the request takes some time to complete, we should start it as soon
                            as possible, and (2) it is better for a long-running process to wait
                            for a short one, rather than the other way around.
                        </li>
                        <li> <!-- TODO: this wasn't an item before -->
                            As an analogy, suppose you are making an apple pie.  The crust takes
                            5 minutes to prepare, but then it has to chill for half an hour.  It takes
                            20 minutes to prepare the filling.  If you prepare the crust first,
                            you can prepare the filling while the crust is chilling, and you can
                            finish the pie in 35 minutes.  If you prepare the filling first, the
                            process takes 55 minutes.
                        </li>
                    </ul>
                </p>
                <p>
                    Most schedulers use some form of priority-based scheduling,
                    where each process has a priority that can be adjusted up or down
                    over time.  When the scheduler runs, it chooses the runnable process
                    with the highest priority.
                </p>
                <p>
                    Here are some of the factors that determine a process's priority:

                    <ul>
                        <li>
                            A process usually starts with a relatively high priority so it
                            starts running quickly.
                        </li>
                        <li>
                            If a process makes a request and blocks before its time slice is
                            complete, it is more likely to be interactive or I/O-bound, so its
                            priority should go up.
                        </li>
                        <li>
                            If a process runs for an entire time slice, it is more likely to
                            be long-running and CPU-bound, so its priority should go down.
                        </li>
                        <li>
                            If a task blocks for a long time and then becomes ready, it
                            should get a priority boost so it can respond to whatever it was
                            waiting for.
                        </li>
                        <li>
                            If process A is blocked waiting for process B, for example if
                            they are connected by a pipe, the priority of process B should go up.
                        </li>
                        <li>
                            The system call <c>nice</c> allows a process to decrease (but not
                            increase) its own priority, allowing programmers to pass explicit
                            information to the scheduler.
                        </li>
                    </ul>
                </p>
                <p>
                    For most systems running normal workloads, scheduling algorithms
                    don't have a substantial effect on performance.  Simple scheduling
                    policies are usually good enough.
                </p>
            </section>

            <section>
                <title>Real-time scheduling</title>

                <p>
                    However, for programs that interact with the real world, scheduling
                    can be very important.  For example, a program that reads data from
                    sensors and controls motors might have to complete recurring tasks at
                    some minimum frequency and react to external events with some maximum
                    response time.  These requirements are often expressed in terms of
                    <term>tasks</term> that must be completed before <term>deadlines</term>.
                </p>
                <p>
                    Scheduling tasks to meet deadlines is called
                    <term>real-time scheduling</term>.  For some applications,
                    a general-purpose operating system like Linux can be modified to
                    handle real-time scheduling.
                    <idx>real-time scheduling</idx>
                    These modifications might include:
                    <ul>
                        <li>
                            Providing richer APIs for controlling task priorities.
                        </li>
                        <li>
                            Modifying the scheduler to guarantee that the process with
                            highest priority runs within a fixed amount of time.
                        </li>
                        <li>
                            Reorganizing interrupt handlers to guarantee
                            a maximum completion time.
                        </li>
                        <li>
                            Modifying locks and other synchronization mechanisms (coming up
                            in the next chapter) to allow a high-priority task to preempt a
                            lower-priority task.
                        </li>
                        <li>
                            Choosing an implementation of dynamic memory allocation that
                            guarantees a maximum completion time.
                        </li>
                    </ul>
                </p>
                <p>
                    For more demanding applications, especially in domains where real-time
                    response is a matter of life and death,
                    <term>real-time operating systems</term> provide specialized capabilities,
                    often with much simpler designs than general purpose operating systems.
                    <idx>real-time operating system</idx>
                </p>
                <!-- TODO: kernel mode?  signals?  user and system time -->
            </section>
        </chapter>
        <chapter xml:id="threads">
            <title>Threads</title>

            <introduction>
                <p>
                    When I mentioned threads in Section <xref ref="unixps"/>, I said that a thread
                    is a kind of process.  Now I will provide a more careful explanation.
                </p>
                <p>
                    When you create a process, the operating system creates a new address
                    space, which includes the text segment, static segment, and heap; it
                    also creates a new <term>thread of execution</term>, which includes the program
                    counter and other hardware state, and the call stack.
                    <idx>thread</idx>
                </p>
                <p>
                    The processes we have seen so far are <term>single-threaded</term>, which means
                    that only one thread of execution runs in each address space.  In this
                    chapter, you will learn about <term>multi-threaded</term> processes that have
                    multiple threads running in the same address space.
                </p>
                <p>
                    Within a single process, all threads share the same text segment, so
                    they run the same code.  But different threads often run different parts
                    of the code.
                </p>
                <p>
                    And they share the same static segment, so if one thread changes a
                    global variable, other threads see the change.  They also share the heap,
                    so threads can share dynamically-allocated chunks.
                </p>
                <p>
                    But each thread has its own stack, so threads can call functions without
                    interfering with each other.  Usually threads don't access each
                    other's local variables (and sometimes they can't).
                </p>
                <p>
                    The example code for this chapter is in the repository for this book,
                    in a directory named <c>counter</c>.  For information on downloading
                    this code, see <xref ref="code"/>.
                </p>
            </introduction>
            <section>
                <title>Creating threads</title>

                <p>
                    The most popular threading standard used with C is POSIX Threads,
                    or Pthreads for short.  The POSIX standard defines a thread model
                    and an interface for creating and controlling threads.  Most
                    versions of UNIX provide an implementation of Pthreads.
                </p>
                <p>
                    Using Pthreads is like using most C libraries:
                    <ul>
                        <li>
                            You include headers files at the beginning of your program.
                        </li>
                        <li>
                            You write code that calls functions defined by Pthreads.
                        </li>
                        <li>
                            When you compile the program, you link it with the Pthread library.
                        </li>
                    </ul>
                </p>
                <p>
                    For my examples, I include the following headers:
                </p>
                <program language="cpp"><code><![CDATA[
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <semaphore.h>
                ]]></code></program>
                <p>
                    <!-- TODO: this is not correct on modern systems -->
                    The first two are standard; the third is for Pthreads and
                    the fourth is for semaphores.  To compile with the Pthread library
                    in <c>gcc</c>, you can use the <c>-l</c> option on the command line:
                </p>
                <console><input>gcc -g -O2 -o array array.c -lpthread</input></console>
                <p>
                    This compiles a source file named <c>array.c</c> with debugging info
                    and optimization, links with the Pthread library, and generates an
                    executable named <c>array</c>.
                </p>
            </section>
            <section>
                <title>Creating threads</title>
                <p>
                    The Pthread function that creates threads is called <c>pthread_create</c>.
                    The following function shows how to use it:
                </p>
                <program language="cpp"><code><![CDATA[
                    pthread_t make_thread(void *(*entry)(void *), Shared *shared)
                    {
                        int n;
                        pthread_t thread;

                        n = pthread_create(&thread, NULL, entry, (void *)shared);
                        if (n != 0) {
                            perror("pthread_create failed");
                            exit(-1);
                        }
                        return thread;
                    }
                    ]]>
                </code></program>
                <p>
                    <c>make_thread</c> is a wrapper I wrote to make
                    <c>pthread_create</c> easier to use, and to provide error-checking.
                </p>
                <p>
                    The return type from <c>pthread_create</c> is <c>pthread_t</c>,
                    which you can think of as an id or <q>handle</q> for the new thread.  
                </p>
                <p>
                    If <c>pthread_create</c> succeeds, it returns 0 and <c>make_thread</c>
                    returns the handle of the new thread.
                    If an error occurs, <c>pthread_create</c>
                    returns an error code and <c>make_thread</c> prints an error message
                    and exits.
                </p>
                <p>
                    The parameters of <c>make_thread</c> take some
                    explaining.  Starting with the second, <c>Shared</c>
                    is a structure I defined to contain values shared between threads.
                    The following <c>typedef</c> statement creates the new type:
                </p>
                <program language="cpp"><code>
                    typedef struct {
                        int counter;
                    } Shared;
                </code></program>
                <p>
                    In this case, the only shared variable is <c>counter</c>.
                    <c>make_shared</c> allocates
                    space for a <c>Shared</c> structure and initializes the contents:
                </p>
                <program language="cpp"><code>
                    Shared *make_shared() {
                        Shared *shared = check_malloc(sizeof (Shared));
                        shared->counter = 0;
                        return shared;
                     }
                </code></program>
                <p>
                    Now that we have a shared data structure, let's get back to
                    <c>make_thread</c>.
                    The first parameter is a pointer to a function that takes
                    a <c>void</c> pointer and returns a <c>void</c> pointer.  If the syntax
                    for declaring this type makes your eyes bleed, you are not alone.
                    Anyway, the purpose of this parameter is to specify the function where
                    the execution of the new thread will begin.  By convention, this
                    function is named <c>entry</c>:
                </p>
                <program language="cpp"><code>
                    void *entry(void *arg) {
                        Shared *shared = (Shared *) arg;
                        child_code(shared);
                        pthread_exit(NULL);
                    }
                </code></program>
                <p>
                    The parameter of <c>entry</c> has to be declared as a <c>void</c>
                    pointer, but in this program we know that it is really a pointer to a
                    <c>Shared</c> structure, so we can typecast it accordingly and then
                    pass it along to <c>child_code</c>, which does the real work.
                </p>
                <p>
                    As a simple example, <c>child_code</c> prints the value of
                    the shared counter and increments it.
                </p>
                <program language="cpp"><code>
                    void child_code(Shared *shared) {  
                        printf("counter = %d\n", shared->counter);
                        shared->counter++;
                    }
                </code></program>
                <p>
                    When <c>child_code</c> returns, <c>entry</c> invokes
                    <c>pthread_exit</c> which can be used to pass a value to the thread
                    that joins with this thread.  In this case, the child has nothing to
                    say, so we pass <c>NULL</c>.
                </p>
                <p>
                    Finally, here is the code that creates the child threads:
                </p>
                <program language="cpp"><code><![CDATA[
                    int i;
                    pthread_t child[NUM_CHILDREN];

                    Shared *shared = make_shared(1000000);

                    for (i = 0; i < NUM_CHILDREN; i++) {
                        child[i] = make_thread(entry, shared);
                    }
                ]]></code></program>
                <p>
                    <c>NUM_CHILDREN</c> is a compile-time constant that determines
                    the number of child threads.  <c>child</c> is an array of
                    thread handles.
                </p>
            </section>
            <section>
                <title>Joining threads</title>

                <p>
                    When one thread wants to wait for another thread to complete,
                    it invokes <c>pthread_join</c>.
                    Here is my wrapper for <c>pthread_join</c>.
                </p>
                <program language="cpp"><code>
                    void join_thread(pthread_t thread) {
                        int ret = pthread_join(thread, NULL);
                        if (ret == -1) {
                            perror("pthread_join failed");
                            exit(-1);
                        }
                    }
                </code></program>
                <p>
                    The parameter is the handle of the thread you want to wait for.
                    All the wrapper does is call <c>pthread_join</c> and check the
                    result.
                </p>
                <p>
                    Any thread can join any other thread, but in the most common pattern
                    the parent thread creates and joins all child threads.
                    Continuing the example from the previous section, here's the
                    code that waits on the children:
                </p>
                <program language="cpp"><code><![CDATA[
                    for (i = 0; i < NUM_CHILDREN; i++) {
                        join_thread(child[i]);
                    }
                ]]></code></program>
                <p>
                    This loops waits for the children one at a time in the order they
                    were created.  There is no guarantee that the child threads complete 
                    in that order, but this loop works correctly even if they don't.  If one
                    of the children is late, the loop might have to wait, and other children
                    might complete in the meantime.  But regardless, the loop exits
                    only when all children are done.
                </p>
                <p>
                    If you have downloaded the repository for this book (see
                    <xref ref="code"/>), you'll find this example in
                    <c>counter/counter.c</c>.  You can compile and run it like this:
                </p>
                <console>
                    <!-- TODO: fix -lpthread -->
                    <input>make counter</input>
                    <output>gcc -Wall counter.c -o counter -lpthread</output>
                    <input>./counter</input>
                </console>
                <p>
                    When I ran it with 5 children, I got the following output:
                </p>
                <console>
                    <output>
counter = 0
counter = 0
counter = 1
counter = 0
counter = 3
                    </output>
                </console>
                <p>
                    When you run it, you will probably get different results.  And if
                    you run it again, you might get different results each time.  What's
                    going on?
                </p>
            </section>

            <section>
                <title>Synchronization errors</title>

                <p>
                    The problem with the previous program is that the children
                    access the shared variable, <c>counter</c>, without synchronization,
                    so several threads can read the same value of <c>counter</c> before
                    any threads increment it.
                </p>
                <p>
                    Here is a sequence of events that could explain the output in the
                    previous section:
                </p>
                <console><output>
Child A reads 0
Child B reads 0
Child C reads 0
Child A prints   0
Child B prints   0
Child A sets counter=1
Child D reads 1
Child D prints   1
Child C prints   0
Child A sets counter=1
Child B sets counter=2
Child C sets counter=3
Child E reads 3
Child E prints   3
Child D sets counter=4
Child E sets counter=5
                </output></console>
                <p>
                    Each time you run the program, threads might be interrupted at different
                    points, or the scheduler might choose different threads to run, so
                    the sequence of events, and the results, will be different.
                </p>
                <p>
                    Suppose we want to impose some order.  For example, we might want
                    each thread to read a different value of <c>counter</c> and increment
                    it, so that the value of <c>counter</c> reflects the number of
                    threads that have executed <c>child_code</c>.
                </p>
                <p>
                    To enforce that requirement, we can use a <term>mutex</term>, which is
                    an object that guarantees <term>mutual exclusion</term> for a block of code;
                    that is, only one thread can execute the block at a time.
                    <idx>mutex</idx>
                    <idx>mutual exclusion</idx>
                </p>
                <p>
                    I have written a small module called <c>mutex.c</c> that provides
                    mutex objects.  I'll show you how to use it first; then I'll explain
                    how it works.
                </p>
                <p>
                    Here's a version of <c>child_code</c> that uses a mutex to synchronize
                    threads:
                </p>
                <program language="cpp"><code><![CDATA[
                    void child_code(Shared *shared) {
                        mutex_lock(shared->mutex);
                        printf("counter = %d\n", shared->counter);
                        shared->counter++;
                        mutex_unlock(shared->mutex);
                    }
                ]]></code></program>
                <p>
                    Before any thread can access <c>counter</c>, it has to <term>lock</term>
                    the mutex, which has the effect of barring all other threads.
                    Suppose Thread A has locked the mutex and is in the
                    middle of <c>child_code</c>.  If Thread B arrives and
                    executes <c>mutex_lock</c>, it blocks.
                </p>
                <p>
                    When Thread A is done, it executes <c>mutex_unlock</c>,
                    which allows Thread B to proceed.  In effect, the threads
                    line up to execute <c>child_code</c> one at a time, so they
                    can't interfere with each other.  When I run this code with
                    5 children, I get:
                </p>
                <console><output>
                    counter = 0
                    counter = 1
                    counter = 2
                    counter = 3
                    counter = 4
                </output></console>
                <p>
                    And that satisfies the requirements.  In order for this solution to
                    work, I have to add the Mutex to the Shared struct:
                </p>
                <program language="cpp"><code>
                    typedef struct {
                        int counter;
                        Mutex *mutex;
                    } Shared;
                </code></program>
                <p>
                    And initialize it in <c>make_shared</c>:
                </p>
                <program language="cpp"><code>
                    Shared *make_shared(int end) {
                        Shared *shared = check_malloc(sizeof(Shared));
                        shared->counter = 0;
                        shared->mutex = make_mutex(); //-- this line is new
                        return shared;
                    }
                </code></program>
                <p>
                    The code in this section is in <c>counter_mutex.c</c>.
                    The definition of <c>Mutex</c> is in <c>mutex.c</c>, which I
                    explain in the next section.
                </p>
            </section>
            <section>
                <title>Mutex</title>

                <p>
                    My definition of <c>Mutex</c> is a wrapper for a type called
                    <c>pthread_mutex_t</c>, which is defined in the POSIX threads API.
                </p>
                <p>
                    To create a POSIX mutex, you have to allocate space for a
                    <c>pthread_mutex_t</c> type and then call <c>pthread_mutex_init</c>.
                </p>
                <p>
                    One of the problems with this API is that <c>pthread_mutex_t</c>
                    behaves like a structure, so if you pass it as an argument, it makes a
                    copy, which makes the mutex behave incorrectly.  To avoid that, you have to
                    pass <c>pthread_mutex_t</c> by address.
                </p>
                <p>
                    My code makes it easier to get that right.  It defines a
                    type, <c>Mutex</c>, which is just a more readable name for
                    <c>pthread_mutex_t</c>:
                </p>
                <program language="cpp"><code><![CDATA[
                    #include <pthread.h>

                    typedef pthread_mutex_t Mutex;
                ]]></code></program>
                <p>
                    Then it defines <c>make_mutex</c>, which allocates space and
                    initializes the mutex:
                </p>
                <program language="cpp"><code>
                    Mutex *make_mutex() {
                        Mutex *mutex = check_malloc(sizeof(Mutex));
                        int n = pthread_mutex_init(mutex, NULL);
                        if (n != 0)
                            perror_exit("make_lock failed"); 
                        return mutex;
                        }
                </code></program>
                <p>
                    The return value is a pointer, which you can pass around as an
                    argument without causing unwanted copying.
                </p>
                <p>
                    The functions to lock and unlock the mutex are simple wrappers
                    for POSIX functions:
                </p>
                <program language="cpp"><code>
                    void mutex_lock(Mutex *mutex) {
                        int n = pthread_mutex_lock(mutex);
                        if (n != 0)
                            perror_exit("lock failed");
                    }

                    void mutex_unlock(Mutex *mutex) {
                        int n = pthread_mutex_unlock(mutex);
                        if (n != 0)
                            perror_exit("unlock failed");
                    }
                </code></program>
                <p>
                    This code is in <c>mutex.c</c> and the header file <c>mutex.h</c>.
                </p>
            </section>
        </chapter>

        <chapter xml:id="conditionvariables">
            <title>Condition variables</title>

            <introduction>
                <p>
                    Many simple synchronization problems can be solved using mutexes
                    as shown in the previous chapter.  In this chapter I introduce a
                    bigger challenge, the well-known <term>Producer-Consumer problem</term>, and
                    a new tool to solve it, the condition variable.
                    <idx>producer-consumer problem</idx>
                </p>
            </introduction>
            <section xml:id="queue">
                <title>The work queue</title>

                <p>
                    In some multi-threaded programs, threads are organized to perform
                    different tasks.  Often they communicate with each other using a queue,
                    where some threads, called <term>producers</term>, put data into the queue
                    and other threads, called <term>consumers</term>, take data out.
                    <idx>producer</idx>
                    <idx>consumer</idx>
                </p>
                <p>
                    For example, in applications with a graphical user interface, there
                    might be one thread that runs the GUI, responding to user events,
                    and another thread that processes user requests.  In that case,
                    the GUI thread might put requests into a queue and the <q>back end</q>
                    thread might take requests out and process them.
                </p>
                <p>
                    To support this organization, we need a queue implementation that is
                    <term>thread safe</term>, which means that both threads (or more than two) can
                    access the queue at the same time.  And we need to handle the special
                    cases when the queue is empty and, if the size of the queue is
                    bounded, when the queue is full.
                    <idx>thread safe</idx>
                </p>
                <p>
                    I'll start with a simple queue that is not thread safe, then we'll see
                    what goes wrong and fix it.  The code for this example is in the
                    repository for this book, in a folder called <c>queue</c>.  The file
                    <c>queue.c</c> contains a basic implementation of a circular buffer,
                    which you can read about at
                    <url href="https://en.wikipedia.org/wiki/Circular_buffer">Wikipedia:Circular buffer</url>.
                </p>
                <p>
                    Here's the structure definition:
                </p>
                <program language="cpp"><code>
                    typedef struct {
                        int *array;
                        int length;
                        int next_in;
                        int next_out;
                    } Queue;
                </code></program>
                <p>
                    <c>array</c> is the array that contains the elements of the queue.
                    For this example the elements are ints, but more generally
                    they would be structures that contain user events, items of work, etc.
                </p>
                <p>
                    <c>length</c> is the length of the array.  <c>next_in</c> is an
                    index into the array that indices where the next element should be
                    added; similarly, <c>next_out</c> is the index of the next element
                    that should be removed.
                </p>
                <p>
                    <c>make_queue</c> allocates space for this structure and initializes
                    the fields:
                </p>
                <program language="cpp"><code>
                    Queue *make_queue(int length) {
                        Queue *queue = (Queue *) malloc(sizeof(Queue));
                        queue->length = length + 1;
                        queue->array = (int *) malloc(length * sizeof(int));
                        queue->next_in = 0;
                        queue->next_out = 0;
                        return queue;
                    }
                </code></program>    
                <p>
                    The initial value for <c>next_out</c> needs some explaining.
                    Since the queue is initially empty, there is no next element to
                    remove, so <c>next_out</c> is invalid.  Setting
                    <c>next_out == next_in</c> is a special case that indicates
                    that the queue is empty, so we can write:
                </p>
                <program language="cpp"><code>
                    int queue_empty(Queue *queue) {
                        return (queue->next_in == queue->next_out);
                    }
                </code></program>
                <p>
                    Now we can add elements to the queue using <c>queue_push</c>:
                </p>
                <program language="cpp"><code>
                    void queue_push(Queue *queue, int item) {
                        if (queue_full(queue)) {
                            perror_exit("queue is full");
                        }
  
                        queue->array[queue->next_in] = item;
                        queue->next_in = queue_incr(queue, queue->next_in);
                    }
                </code></program>
                <p>
                    If the queue is full, <c>queue_push</c> prints an error message
                    and exits.  I will explain <c>queue_full</c> soon.
                </p>
                <p>
                    If the queue is not full, <c>queue_push</c> inserts the new
                    element and then increments <c>next_in</c> using <c>queue_incr</c>:
                </p>
                <program language="cpp"><code>
                    int queue_incr(Queue *queue, int i) {
                        return (i+1) % queue->length;
                    }
                </code></program>
                <p>
                    When the index, <c>i</c>, gets to the end of the array, it wraps around
                    to 0.  And that's where we run into a tricky part.  If we keep adding
                    elements to the queue, eventually <c>next_in</c> wraps around and catches
                    up with <c>next_out</c>.  But if <c>next_in == next_out</c>, we would
                    incorrectly conclude that the queue was empty.
                </p>
                <p>
                    To avoid that, we define another special case to indicate that the
                    queue is full:
                </p>
                <program language="cpp"><code>
                    int queue_full(Queue *queue) {
                        return (queue_incr(queue, queue->next_in) == queue->next_out);
                    }
                </code></program>
                <p>
                    If incrementing <c>next_in</c> lands on <c>next_out</c>, that means
                    we can't add another element without making the queue seem empty.  So
                    we stop one element before the <q>end</q> (keeping in mind that the end of
                    the queue can be anywhere, not necessarily the end of the array).
                </p>
                <p>
                    Now we can write <c>queue_pop</c>, which removes and returns the next
                    element from the queue:
                </p>
                <program language="cpp"><code>
                    int queue_pop(Queue *queue) {
                        if (queue_empty(queue)) {
                            perror_exit("queue is empty");
                        }
                        int item = queue->array[queue->next_out];
                        queue->next_out = queue_incr(queue, queue->next_out);
                        return item;
                    }
                </code></program>
                <p>
                    If you try to pop from an empty queue, <c>queue_pop</c> prints
                    an error message and exits.
                </p>
            </section>

            <section xml:id="prodcon">
                <title>Producers and consumers</title>

                <p>
                    Now let's make some threads to access this queue.  Here's the
                    producer code:
                </p>
                <program language="cpp"><code><![CDATA[
                    void *producer_entry(void *arg) {
                        Shared *shared = (Shared *) arg;

                        for (int i = 0; i < QUEUE_LENGTH-1; i++) {
                            printf("adding item %d\n", i);
                            queue_push(shared->queue, i);
                        }
                        pthread_exit(NULL);
                    }
                ]]></code></program>
                <p>
                    Here's the consumer code:
                </p>
                <program language="cpp"><code><![CDATA[
                    void *consumer_entry(void *arg) {
                        int item;
                        Shared *shared = (Shared *) arg;

                        for (int i = 0; i < QUEUE_LENGTH-1; i++) {
                            item = queue_pop(shared->queue);
                            printf("consuming item %d\n", item);
                        }
                        pthread_exit(NULL);
                    }
                ]]></code></program>
                <p>
                    Here's the parent code that starts the threads and waits for them:
                </p>
                <program language="cpp"><code><![CDATA[
                    pthread_t child[NUM_CHILDREN];

                    Shared *shared = make_shared();

                    child[0] = make_thread(producer_entry, shared);
                    child[1] = make_thread(consumer_entry, shared);

                    for (int i = 0; i < NUM_CHILDREN; i++) {
                        join_thread(child[i]);
                    }
                ]]></code></program>
                <p>
                    And finally here's the shared structure that contains the queue:
                </p>
                <program language="cpp"><code><![CDATA[
                    typedef struct {
                        Queue *queue;
                    } Shared;

                    Shared *make_shared() {
                        Shared *shared = check_malloc(sizeof(Shared));
                        shared->queue = make_queue(QUEUE_LENGTH);
                        return shared;
                    }
                ]]></code></program>
                <p>
                    The code we have so far is a good starting place, but it has
                    several problems:
                    <ul>
                        <li>
                            Access to the queue is not thread safe.  Different threads
                            could access <c>array</c>, <c>next_in</c>, and <c>next_out</c>
                            at the same time and leave the queue in a broken, <q>inconsistent</q>
                            state.
                        </li>
                        <li>
                            If the consumer is scheduled first, it finds the queue empty,
                            print an error message, and exits.  We would rather have the consumer
                            block until the queue is not empty.  Similarly, we would like the
                            producer to block if the queue is full.
                        </li>
                    </ul>
                </p>
                <p>
                    In the next section, we solve the first problem with a <c>Mutex</c>.
                    In the following section, we solve the second problem with condition
                    variables.
                </p>
            </section>
            <section>
                <title>Mutual exclusion</title>

                <p>
                    We can make the queue thread safe with a mutex.  This version
                    of the code is in <c>queue_mutex.c</c>.
                </p>
                <p>
                    First we add a <c>Mutex</c> pointer to the queue structure:
                </p>
                <program language="cpp"><code>
                    typedef struct {
                        int *array;
                        int length;
                        int next_in;
                        int next_out;
                        Mutex *mutex; //-- this line is new
                    } Queue;
                </code></program>
                <p>
                    And initialize the <c>Mutex</c> in <c>make_queue</c>:
                </p>
                <program language="cpp"><code>
                    Queue *make_queue(int length) {
                        Queue *queue = (Queue *) malloc(sizeof(Queue));
                        queue->length = length;
                        queue->array = (int *) malloc(length * sizeof(int));
                        queue->next_in = 0;
                        queue->next_out = 0;
                        queue->mutex = make_mutex();   //-- new
                        return queue;
                    }
                </code></program>
                <p>
                    Next we add synchronization code to <c>queue_push</c>:
                </p>
                <program language="cpp"><code>
                    void queue_push(Queue *queue, int item) {
                        mutex_lock(queue->mutex);   //-- new
                        if (queue_full(queue)) {
                            mutex_unlock(queue->mutex);   //-- new
                            perror_exit("queue is full");
                        }
  
                        queue->array[queue->next_in] = item;
                        queue->next_in = queue_incr(queue, queue->next_in);
                        mutex_unlock(queue->mutex);   //-- new
                    }
                </code></program>
                <p>
                    Before checking whether the queue is full, we have to lock
                    the <c>Mutex</c>.  If the queue is full, we have to unlock
                    the <c>Mutex</c> before exiting; otherwise the thread would leave
                    it locked and no other threads could proceed.
                </p>
                <p>
                    The synchronization code for <c>queue_pop</c> is similar:
                </p>
                <program language="cpp"><code>
                    int queue_pop(Queue *queue) {
                        mutex_lock(queue->mutex);
                        if (queue_empty(queue)) {
                            mutex_unlock(queue->mutex);
                            perror_exit("queue is empty");
                        }
  
                        int item = queue->array[queue->next_out];
                        queue->next_out = queue_incr(queue, queue->next_out);
                        mutex_unlock(queue->mutex);
                        return item;
                    }
                </code></program>
                <p>
                    Note that the other <c>Queue</c> functions, <c>queue_full</c>,
                    <c>queue_empty</c>, and <c>queue_incr</c> do not try to lock
                    the mutex.  Any thread that calls these functions is required to
                    lock the mutex first; this requirement is part of the documented
                    interface for these functions.
                </p>
                <p>
                    With this additional code, the queue is thread safe; if you run it, you
                    should not see any synchronization errors.  But it is likely
                    that the consumer will exit at some point because the queue is
                    empty, or the producer will exit because the queue is full,
                    or both.
                </p>
                <p>
                    The next step is to add condition variables.
                </p>
            </section>

            <section>
                <title>Condition variables</title>

                <p>
                    A condition variable is a data structure associated with a condition;
                    it allows threads to block until the condition becomes true.  For
                    example, <c>thread_pop</c> might want check whether the queue is
                    empty and, if so, wait for a condition like <q>queue not empty</q>.
                </p>
                <p>
                    Similarly, <c>thread_push</c> might want to check whether the queue is
                    full and, if so, block until it is not full.
                </p>
                <p>
                    I'll handle the first condition here, and you will have a chance to
                    handle the second condition as an exercise.
                </p>
                <p>
                    First we add a condition variable to the <c>Queue</c> structure:
                </p>
                <program language="cpp"><code>
                    typedef struct {
                        int *array;
                        int length;
                        int next_in;
                        int next_out;
                        Mutex *mutex;
                        Cond *nonempty;   //-- new
                    } Queue;
                </code></program>
                <p>
                    And initialize it in <c>make_queue</c>:
                </p>
                <program language="cpp"><code>
                    Queue *make_queue(int length) {
                        Queue *queue = (Queue *) malloc(sizeof(Queue));
                        queue->length = length;
                        queue->array = (int *) malloc(length * sizeof(int));
                        queue->next_in = 0;
                        queue->next_out = 0;
                        queue->mutex = make_mutex();
                        queue->nonempty = make_cond();   //-- new
                        return queue;
                    }
                </code></program>
                <p>
                    Now in <c>queue_pop</c>, if we find the queue empty, we don't
                    exit; instead we use the condition variable to block:
                </p>
                <program language="cpp"><code>
                    int queue_pop(Queue *queue) {
                        mutex_lock(queue->mutex);
                        while (queue_empty(queue)) {
                            cond_wait(queue->nonempty, queue->mutex);  //-- new
                        }
  
                        int item = queue->array[queue->next_out];
                        queue->next_out = queue_incr(queue, queue->next_out);
                        mutex_unlock(queue->mutex);
                        cond_signal(queue->nonfull);   //-- new
                        return item;
                    }
                </code></program>
                <p>
                    <c>cond_wait</c> is complicated, so let's take it slow.  
                    The first argument is the condition variable; in this case,
                    the condition we are waiting for is <q>queue not empty</q>.  The second
                    argument is the mutex that protects the queue.
                </p>
                <p>
                    When the thread that locked the mutex calls <c>cond_wait</c>, it
                    unlocks the mutex and then blocks.  This is important.  If
                    <c>cond_wait</c> did not unlock the mutex before blocking, no
                    other thread would be able to access the queue, no more items
                    could be added, and the queue would always be empty.
                </p>
                <p>
                    So while the consumer is blocked on <c>nonempty</c>, the producer can
                    run.  Let's see what happens when the producer runs <c>queue_push</c>:
                </p>
                <program language="cpp"><code>
                    void queue_push(Queue *queue, int item) {
                        mutex_lock(queue->mutex);
                        if (queue_full(queue)) {
                            mutex_unlock(queue->mutex);
                            perror_exit("queue is full");
                        }
                        queue->array[queue->next_in] = item;
                        queue->next_in = queue_incr(queue, queue->next_in);
                        mutex_unlock(queue->mutex);
                        cond_signal(queue->nonempty);    //-- new
                    }
                </code></program>
                <p>
                    Just as before, <c>queue_push</c> locks the <c>Mutex</c> and checks
                    whether the queue is full.  Assuming it is not, <c>queue_push</c> adds
                    a new element to the queue and then unlocks the <c>Mutex</c>.
                </p>
                <p>
                    But before returning, it does one more thing: it <q>signals</q> the
                    condition variable <c>nonempty</c>.
                </p>
                <p>
                    Signalling a condition variable usually indicates that the condition is
                    true.  If there are no threads waiting
                    on the condition variable, the signal has no effect.
                </p>
                <p>
                    If there are threads waiting on the condition variable, one of them
                    gets unblocked and resumes execution of <c>cond_wait</c>.  But before
                    the awakened thread can return from <c>cond_wait</c>, it has
                    to wait for and lock the <c>Mutex</c>, again.
                </p>
                <p>
                    Now go back to <c>queue_pop</c> and see what happens when the thread
                    returns from <c>cond_wait</c>.  It loops back to the top of the while
                    loop and checks the condition again.  I'll explain why in just a
                    second, but for now let's assume that the condition is true; that is,
                    the queue is not empty.
                </p>
                <p>
                    When the consumer thread exits the while loop, we know two things: (1)
                    the condition is true, so there is at least one item in the queue, and
                    (2) the <c>Mutex</c> is locked, so it is safe to access the queue.
                </p>
                <p>
                    After removing an item, <c>queue_pop</c> unlocks the mutex
                    and returns.
                </p>
                <p>
                    In the next section I'll show you how my <c>Cond</c> code works, but first I
                    want to answer two frequently-asked questions:
                    <ul>
                        <li>
                            <p>
                                Why is <c>cond_wait</c> inside a while loop rather than an if
                                statement; that is, why do we have to check the condition again after
                                returning from <c>cond_wait</c>?
                            </p>
                            <p>
                                The primary reason you have to re-check the condition is the possibility
                                of an intercepted signal.  Suppose Thread A is waiting on <c>nonempty</c>.
                                Thread B adds an item to the queue and signals <c>nonempty</c>.  Thread
                                A wakes up an tries to lock the mutex, but before it gets the chance,
                                Evil Thread C swoops in, locks the mutex, pops the item from the
                                queue, and unlocks the mutex.  Now the queue is empty again, but
                                Thread A is not blocked any more.  Thread A could lock the mutex and
                                returns from <c>cond_wait</c>.  If Thread A does not check the condition
                                again, it would try to pop an element from an empty queue, and probably
                                cause an error.
                            </p>
                        </li>
                        <li>
                            <p>
                                The other question that comes up when people learn about condition
                                variables is <q>How does the condition variable know what condition it
                                is associated with?</q>
                            </p>
                            <p>
                                This question is understandable because there is no explicit connection
                                between a <c>Cond</c> structure and the condition it relates to.  The
                                connection is implicit in the way it is used.
                            </p>
                            <p>
                                Here's one way to think of it: the condition associated with a Cond
                                is the thing that is false when you call <c>cond_wait</c> and true
                                when you call <c>cond_signal</c>.
                            </p>
                        </li>
                    </ul>
                </p>
                <p>
                    Because threads have to check the condition when they return from
                    <c>cond_wait</c>, it is not strictly necessary to call <c>cond_signal</c>
                    only when the condition is true.  If you have reason to think the
                    condition {\em might} be true, you could call <c>cond_signal</c> as
                    a suggestion that now is a good time to check.
                </p>
            </section>

            <section>
                <title>Condition variable implementation</title>

                <p>
                    The <c>Cond</c> structure I used in the previous section is a wrapper
                    for a type called <c>pthread_cond_t</c>, which is defined in the POSIX
                    threads API.  It is very similar to Mutex, which is a wrapper for
                    <c>pthread_mutex_t</c>.  Both wrappers are defined in <c>utils.c</c> and
                    <c>utils.h</c>.
                </p>
                <p>
                    Here's the typedef:
                </p>
                <program language="cpp"><code>
                    typedef pthread_cond_t Cond;
                </code></program>
                <p>
                    <c>make_cond</c> allocates space, initializes the condition variable,
                    and returns a pointer:
                </p>
                <program language="cpp"><code>
                    Cond *make_cond() {
                        Cond *cond = check_malloc(sizeof(Cond)); 
                        int n = pthread_cond_init(cond, NULL);
                        if (n != 0)
                            perror_exit("make_cond failed");
                        return cond;
                    }
                </code></program>
                <p>
                    And here are the wrappers for <c>cond_wait</c> and <c>cond_signal</c>.
                </p>
                <program language="cpp"><code>
                    void cond_wait(Cond *cond, Mutex *mutex) {
                        int n = pthread_cond_wait(cond, mutex);
                        if (n != 0)
                            perror_exit("cond_wait failed");
                    }

                    void cond_signal(Cond *cond) {
                        int n = pthread_cond_signal(cond);
                        if (n != 0)
                            perror_exit("cond_signal failed");
                    }
                </code></program>
                <p>
                    At this point there should be nothing too surprising there.
                </p>
            </section>
        </chapter>

        <chapter xml:id="semaphores-in-c">
            <title>Semaphores in C</title>

            <introduction>
                <p>
                    Semaphores are a good way to learn about synchronization, but
                    they are not as widely used, in practice, as mutexes and
                    condition variables.
                </p>
                <p>
                    Nevertheless, there are some synchronization problems that can be
                    solved simply with semaphores, yielding solutions that are more
                    demonstrably correct.
                </p>
                <p>
                    This chapter presents a C API for working with semaphores and
                    my code for making it easier to work with.  And it presents
                    a final challenge: can you write an implementation of a semaphore
                    using mutexes and condition variables?
                </p>
                <p>
                    The code for this chapter is in directory <c>semaphore</c> in the
                    repository for this book (see <xref ref="code"/>).
                </p>
            </introduction>

            <section>
                <title>POSIX Semaphores</title>

                <p>
                    A semaphore is a data structure used to help threads work together
                    without interfering with each other.
                </p>
                <p>
                    The POSIX standard specifies an interface for semaphores;
                    it is not part of Pthreads, but most UNIXes
                    that implement Pthreads also provide semaphores.
                </p>
                <p>
                    POSIX semaphores have type <c>sem_t</c>.
                    As usual, I put a wrapper around <c>sem_t</c>
                    to make it easier to use.  The interface is defined in <c>sem.h</c>:
                </p>
                <program language="cpp"><code>
                    typedef sem_t Semaphore;

                    Semaphore *make_semaphore(int value);
                    void semaphore_wait(Semaphore *sem);
                    void semaphore_signal(Semaphore *sem);
                </code></program>
                <p>
                    <c>Semaphore</c> is a synonym for <c>sem_t</c>, but I find it more
                    readable, and the capital letter reminds me to treat it like an
                    object and pass it by pointer.
                </p>
                <p>
                    The implementation of these functions is in <c>sem.c</c>:
                </p>
                <program language="cpp"><code>
                    Semaphore *make_semaphore(int value) {
                        Semaphore *sem = check_malloc(sizeof(Semaphore));
                        int n = sem_init(sem, 0, value);
                        if (n != 0)
                            perror_exit("sem_init failed");
                        return sem;
                    }
                </code></program>
                <p>
                    <c>make_semaphore</c> takes the initial value of the semaphore
                    as a parameter.  It allocates space for a Semaphore, initializes
                    it, and returns a pointer to <c>Semaphore</c>.
                </p>
                <p>
                    <c>sem_init</c> returns 0 if it succeeds and -1 if anything goes
                    wrong.  One nice thing about using wrapper functions is that you can
                    encapsulate the error-checking code, which makes the code that uses
                    these functions more readable.
                </p>
                <p>
                    Here is the implementation of <c>semaphore_wait</c>:
                </p>
                <program language="cpp"><code>
                    void semaphore_wait(Semaphore *sem) {
                        int n = sem_wait(sem);
                        if (n != 0)
                            perror_exit("sem_wait failed");
                    }
                </code></program>
                <p>
                    And here is <c>semaphore_signal</c>:
                </p>
                <program language="cpp"><code>
                    void semaphore_signal(Semaphore *sem) {
                        int n = sem_post(sem);
                        if (n != 0)
                            perror_exit("sem_post failed");
                    }
                </code></program>
                <p>
                    I prefer to call this operation <q>signal</q> rather than <q>post</q>,
                    although both terms are common.
                </p>
                <p>
                    Here's an example that shows how to use a semaphore as a mutex:
                </p>
                <program language="cpp"><code>
                    Semaphore *mutex = make_semaphore(1);

                    semaphore_wait(mutex);
                      // protected code goes here
                    semaphore_signal(mutex);
                </code></program>
                <p>
                    When you use a semaphore as a mutex, you usually
                    initialize it to 1 to indicate
                    that the mutex is unlocked; that is, one thread can
                    pass the semaphore without blocking.
                </p>
                <p>
                    Here I am using the variable name <c>mutex</c> to indicate that
                    the semaphore is being used as a mutex.  But remember that the behavior
                    of a semaphore is not the same as a Pthread mutex.
                </p>
            </section>
            <section>
                <title>Producers and consumers with semaphores</title>

                <p>
                    Using these semaphore wrapper functions, we can
                    write a solution to the Producer-Consumer problem from
                    <xref ref="prodcon"/>.
                    The code in this section is in <c>queue_sem.c</c>.
                </p>
                <p>
                    Here's the new definition of <c>Queue</c>, replacing the mutex
                    and condition variables with semaphores:
                </p>
                <program language="cpp"><code>
                    typedef struct {
                        int *array;
                        int length;
                        int next_in;
                        int next_out;
                        Semaphore *mutex;       //-- new
                        Semaphore *items;       //-- new
                        Semaphore *spaces;      //-- new
                    } Queue;
                </code></program>
                <p>
                    And here's the new version of <c>make_queue</c>:
                </p>
                <program language="cpp"><code>
                    Queue *make_queue(int length) {
                        Queue *queue = (Queue *) malloc(sizeof(Queue));
                        queue->length = length;
                        queue->array = (int *) malloc(length * sizeof(int));
                        queue->next_in = 0;
                        queue->next_out = 0;
                        queue->mutex = make_semaphore(1);
                        queue->items = make_semaphore(0);
                        queue->spaces = make_semaphore(length-1);
                        return queue;
                    }
                </code></program>
                <p>
                    <c>mutex</c> is used to guarantee exclusive access to the queue;
                    the initial value is 1, so the mutex is
                    initially unlocked.
                </p>
                <p>
                    <c>items</c> is the number of items in the queue, which is also the number
                    of consumer threads that can execute <c>queue_pop</c> without blocking.
                    Initially there are no items in the queue.
                </p>
                <p>
                    <c>spaces</c> is the number of empty spaces in the queue, which is the
                    number of producer threads that can execute <c>queue_push</c> without
                    blocking.  Initially the number of spaces is the capacity of the queue,
                    which is <c>length-1</c>, as explained in Section~\ref{queue}.
                </p>
                <p>
                    Here is the new version of <c>queue_push</c>, which is run by
                    producer threads:
                </p>
                <program language="cpp"><code>
                    void queue_push(Queue *queue, int item) {
                        semaphore_wait(queue->spaces);
                        semaphore_wait(queue->mutex);

                        queue->array[queue->next_in] = item;
                        queue->next_in = queue_incr(queue, queue->next_in);

                        semaphore_signal(queue->mutex);
                        semaphore_signal(queue->items);
                    }
                </code></program>
                <p>
                    Notice that <c>queue_push</c> doesn't have to call
                    <c>queue_full</c> any more; instead, the semaphore keeps track of
                    how many spaces are available and blocks producers if the queue
                    is full.
                </p>
                <p>
                    Here is the new version of <c>queue_pop</c>:
                </p>
                <program language="cpp"><code>
                    int queue_pop(Queue *queue) {
                        semaphore_wait(queue->items);
                        semaphore_wait(queue->mutex);
  
                        int item = queue->array[queue->next_out];
                        queue->next_out = queue_incr(queue, queue->next_out);

                        semaphore_signal(queue->mutex);
                        semaphore_signal(queue->spaces);

                        return item;
                    }
                </code></program>
                <p>
                    This solution is explained, using pseudo-code, in Chapter 4 of
                    <pubtitle>The Little Book of Semaphores</pubtitle>.
                </p>
                <p>
                    Using the code in the repository for this book, you should be
                    able to compile and run this solution like this:
                </p>
                <console>
                    <input>make queue_sem</input>
                    <input>./queue_sem</input>
                </console>
            </section>

            <section xml:id="makeyourown">
                <title>Make your own semaphores</title>
                <introduction>
                    <p>
                        Any problem that can be solved with semaphores can also be solved
                        with condition variables and mutexes.  We can prove that's true
                        by using condition variables and mutexes to implement a semaphore.
                    </p>
                    <p>
                        Before you go on, you might want to try this as an exercise: write
                        functions that implement the semaphore API in <c>sem.h</c>
                        using using condition variables and mutexes.  In the repository for
                        this book, you'll find my solution in <c>mysem_soln.c</c> and
                        <c>mysem_soln.h</c>.
                    </p>
                    <p>
                        If you have trouble getting started, you can use the following
                        structure definition, from my solution, as a hint:
                    </p>
                    <program language="cpp"><code>
                        typedef struct {
                            int value, wakeups;
                            Mutex *mutex;
                            Cond *cond;
                        } Semaphore;
                    </code></program>
                    <!-- TODO: Include Property 3 (it's in LBoS). -->
                    <p>
                        <c>value</c> is the value of the semaphore.  <c>wakeups</c> counts
                        the number of pending signals; that is, the number of threads
                        that have been woken but have not yet resumed execution.  The reason
                        for wakeups is to make sure that our semaphores have
                        Property 3, described in <c>The Little Book of Semaphores</c>.
                    </p>
                    <p>
                        <c>mutex</c> provides exclusive access to <c>value</c> and
                        <c>wakeups</c>; <c>cond</c> is the condition variable threads
                        wait on if they wait on the semaphore.
                    </p>
                    <p>
                        Here is the initialization code for this structure:
                    </p>
                    <program language="cpp"><code>
                        Semaphore *make_semaphore(int value) {
                            Semaphore *semaphore = check_malloc(sizeof(Semaphore));
                            semaphore->value = value;
                            semaphore->wakeups = 0;
                            semaphore->mutex = make_mutex();
                            semaphore->cond = make_cond();
                            return semaphore;
                        }
                    </code></program>
                </introduction>
                <subsection>
                    <title>Semaphore implementation</title>

                    <p>
                        Here is my implementation of semaphores using POSIX mutexes
                        and condition variables:
                    </p>
                    <program language="cpp"><code><![CDATA[
                        void semaphore_wait(Semaphore *semaphore) {
                            mutex_lock(semaphore->mutex);
                            semaphore->value--;

                            if (semaphore->value < 0) {
                                do {
                                    cond_wait(semaphore->cond, semaphore->mutex);
                                } while (semaphore->wakeups < 1);
                                semaphore->wakeups--;
                            }
                            mutex_unlock(semaphore->mutex);
                        }
                    ]]></code></program>
                    <p>
                        When a thread waits on the semaphore, it has to lock the mutex
                        before it decrements <c>value</c>.  If the value of the semaphore
                        becomes negative, the thread blocks until a <q>wakeup</q> is
                        available.  While it is blocked, the mutex is unlocked, so another
                        thread can signal.
                    </p>
                    <p>
                        Here is the code for <c>semaphore_signal</c>:
                    </p>
                    <program language="cpp"><code><![CDATA[
                        void semaphore_signal(Semaphore *semaphore) {
                            mutex_lock(semaphore->mutex);
                            semaphore->value++;

                            if (semaphore->value <= 0) {
                                semaphore->wakeups++;
                                cond_signal(semaphore->cond);
                            }
                            mutex_unlock(semaphore->mutex);
                        }
                    ]]></code></program>
                    <p>
                        Again, a thread has to lock the mutex before it increments
                        <c>value</c>.  If the semaphore was negative, that means threads
                        are waiting, so the signalling thread increments <c>wakeups</c> and
                        signals the condition variable.
                    </p>
                    <p>
                        At this point one of the waiting threads might wake up, but the
                        mutex is still locked until the signalling thread unlocks it.
                    </p>
                    <p>
                        At that point, one of the waiting threads returns from <c>cond_wait</c>
                        and checks whether a wakeup is still available.  If not, it
                        loops and waits on the condition variable again.  If so, it
                        decrements <c>wakeups</c>, unlocks the mutex, and exits.
                    </p>
                    <p>
                        One thing about this solution that might not be obvious is the use of
                        a <c>do...while</c> loop.  Can you figure out why it is not a
                        more conventional <c>while</c> loop?  What would go wrong?
                    </p>
                    <p>
                        The problem is that with a <c>while</c> loop this implementation would
                        not have Property 3.  It would be possible for a thread to signal and
                        then run around and catch its own signal.
                    </p>
                    <p>
                        With the <c>do...while</c> loop, it is guaranteed
                        <fn>Well, almost.  It turns out that a well-timed spurious wakeup (see
                        <url href="https://en.wikipedia.org/wiki/Spurious_wakeup">Wikipedia:Spurious wakup</url>)
                        can violate this guarantee.</fn> that when a thread signals, one of the waiting threads
                        will get the signal, even if the signalling thread runs around and
                        gets the mutex before one of the waiting threads resumes.
                    </p>
                </subsection>
            </section>
        </chapter>
        <backmatter>
            <index>
                <index-list/>
            </index>
        </backmatter>
    </book>
</pretext>
